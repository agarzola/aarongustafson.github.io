<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
	<title><![CDATA[Aaron Gustafson - Latest Posts]]></title>
	<link href="https://www.aaron-gustafson.com/atom-latest-posts.xml" rel="self"/>
	<link href="https://www.aaron-gustafson.com/"/>
	<updated>2015-09-17T15:57:12-04:00</updated>
	<id>https://www.aaron-gustafson.com/</id>
	<author>
		<name><![CDATA[Aaron Gustafson]]></name>
		
	</author>
	<generator uri="http://octopress.org/">Octopress</generator>

	
		
	
		
	
		
	
		
	
		
	
		
	
		
	
		
			<entry>
				<title type="html"><![CDATA[Moved to HTTPS]]></title>
				<link href="https://www.aaron-gustafson.com/notebook/moved-to-https/"/>
				<updated>2015-09-03T16:06:03-04:00</updated>
				<id>https://www.aaron-gustafson.com/notebook/moved-to-https</id>
				<content type="html"><![CDATA[<p>I’ve been complaining about <a href="https://www.aaron-gustafson.com/notebook/more-proof-we-dont-control-our-web-pages/">“man in the middle” attacks brought on by internet service providers</a> a bunch over the last year. The only way to keep uninvited third parties from injecting JavaScript and more—potentially screwing up your page—is to move to HTTPS. So, as much as it pains me to abandon good old fashioned HTTP, I’ve decided to lock things down a bit.</p>

<!-- more -->

<p>I was using <a href="https://github.com/">Github</a> to host my site as a <a href="https://pages.github.com/">Github page</a>. It worked really well given this is a static site, but you can’t run Github-hosted sites under HTTPS unless you go with their <code>*.github.io</code> domain name (they have a <a href="https://en.wikipedia.org/wiki/Wildcard_certificate">wildcard certificate</a> for that domain). There’s been <a href="https://github.com/isaacs/github/issues/156">a ton of interest in Github allowing custom cert installation, but no movement yet</a>, so… <i>onward!</i><sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup></p>

<p>I opted to move to <a href="https://www.digitalocean.com/?refcode=5270a681c6fe">DigitalOcean</a> since <a href="http://easy-designs.net">my consultancy</a> recently relocated all of its sites there in a mass exodus from MediaTemple. Migrating the site was as simple as <a href="https://www.digitalocean.com/community/tutorials/how-to-set-up-automatic-deployment-with-git-with-a-vps">setting up the DigitalOcean server as a new “live” <code>remote</code> on my local git install</a> and pushing it up there. Since it’s a static site, I didn’t have to worry too much about the server config. Apache is really great at hosting static files.</p>

<p>With the contents in place, I went through <a href="https://www.digitalocean.com/community/tutorials/how-to-set-up-apache-with-a-free-signed-ssl-certificate-on-a-vps">the rather convoluted process of getting SSL set up following the instructions from DigitalOcean</a>. I opted for the free <a href="http://www.startssl.com/">StartSSL</a> certificate to begin with (a rather convoluted process, but we got there in the end) and then flipped the DNS records to point to the new box. Given that the StartSSL certificate needs to be renewed every 30 days, I may opt for a paid certificate in the not too distant future.</p>

<p>Once the DNS propagated, I had to go back and button up a few scripts that were requesting non-HTTPS content. I also had to tweak my Jekyll plugins and Rake tasks to include the legacy “http://” URLs when querying for webmentions and the like (since I didn’t want to lose those references). I also updated the Apache’s <code>VirtualHost</code> configuration for the non-secure site to make all traffic redirect:</p>

<pre><code>Redirect permanent / https://www.aaron-gustafson.com/
</code></pre>

<p>All in all, it was a relatively painless migration. Admittedly, the initial re-build of the site (after updating the Rake tasks) did re-submit all of the webmentions I’d previously sent in order to provide the new address. If I referenced you a bunch in the past, I apologize for the flood of traffic, but it had to be done.</p>

<p>Anyway, so now this site is running under HTTPS. If you encounter any issues, please let me know. And if you want to read a really good account of migrating a site to HTTPS, you should definitely <a href="https://adactio.com/articles/7435">read Jeremy Keith’s step-by-step guide</a>.</p>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>It’s worth noting that <a href="https://github.com/aarongustafson/aarongustafson.github.io/tree/source">the source of the site</a> (and even <a href="https://github.com/aarongustafson/aarongustafson.github.io/tree/master">a back-up build</a>) will remain on Github for the forseeable future. <a href="#fnref:1" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
			</entry>
		
	
		
	
		
	
		
	
		
	
		
			<entry>
				<title type="html"><![CDATA[More Proof We Don’t Control Our Web Pages]]></title>
				<link href="https://www.aaron-gustafson.com/notebook/more-proof-we-dont-control-our-web-pages/"/>
				<updated>2015-09-01T11:33:59-04:00</updated>
				<id>https://www.aaron-gustafson.com/notebook/more-proof-we-dont-control-our-web-pages</id>
				<content type="html"><![CDATA[<p>I’ve talked about this before: As web designers, <a href="http://www.aaron-gustafson.com/notebook/the-network-effect/">we can’t trust the network</a>. Sure, we have to contend with mobile data “dead zones” and dropped connections as our users move about throughout the day, but there’s a lot more to the network that’s beyond our control.</p>

<!-- more -->

<p>Here’s a roundup of some of my “favorite” network issue related headlines from the last few years:</p>

<ul>
  <li><a href="http://www.theguardian.com/technology/2014/jan/28/sky-broadband-blocks-jquery-web-critical-plugin">Sky Broadband misclassified the jQuery CDN as a malware site</a> and broke much of the web for their users.</li>
  <li><a href="http://arstechnica.com/tech-policy/2014/09/why-comcasts-javascript-ad-injections-threaten-security-net-neutrality/">Comcast admitted to injecting self-promotional advertising</a> into web pages served by their Xfinity routers. (They have also been called out for <a href="https://blog.ryankearney.com/2013/01/comcast-caught-intercepting-and-altering-your-web-traffic/">artificially inflating subscriber bandwidth usage with their own crap</a>.)</li>
  <li><a href="http://arstechnica.com/business/2015/08/united-in-flight-wi-fi-reportedly-blocks-ars-technica-and-new-york-times/">United was recently called out for blocking access to the <cite>New York Times</cite></a> on their in-flight Wi-Fi.</li>
  <li><a href="http://webpolicy.org/2015/08/25/att-hotspots-now-with-advertising-injection/">Someone discovered AT&amp;T was injecting CSS, images, and JavaScript into pages</a> served via their airport hotspots.</li>
  <li><a href="http://www.cnet.com/au/news/samsung-smart-tvs-forcing-ads-into-video-streaming-apps/">Samsung smart TVs were found to be injecting video ads</a> into video streaming apps.</li>
  <li><a href="http://pleckey.me/blog/2013/09/11/sprint-mobile-broadband-injecting-3rd-party-javascript/">Sprint injects JavaScript into pages</a> served via its data connections.</li>
  <li><a href="http://www.ecommercetimes.com/story/82117.html">Browser add-ins can inject their own advertisements</a>. They can also alter the DOM, load conflicting versions of JavaScript libraries, and more. Awesome, I know. (This is being addressed, but is a persistent issue when add-ins have the ability to manipulate the DOM.)</li>
</ul>

<p>Some of these issues can be avoided by serving content over HTTPS, but that still won’t enable you to bypass things like firewall blacklists (which led to the jQuery outage on Sky). Your best bet is to design defensively and make sure your users can still accomplish their goals on your site when some resources are missing or markup is altered.</p>

<p>We can’t control what happens to us in this world, we can only control our reaction to it.</p>
]]></content>
			</entry>
		
	
		
	
		
	
		
	
		
	
		
	
		
	
		
	
</feed>
