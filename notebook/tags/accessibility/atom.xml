<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Browse by Tag: Accessibility | Aaron Gustafson]]></title>
  <link href="https://www.aaron-gustafson.com/notebook/tags/accessibility/atom.xml" rel="self"/>
  <link href="https://www.aaron-gustafson.com/"/>
  <updated>2015-10-28T15:30:25-04:00</updated>
  <id>https://www.aaron-gustafson.com/</id>
  <author>
    <name><![CDATA[Aaron Gustafson]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Harvard, MIT, and Captioning]]></title>
    <link href="https://www.aaron-gustafson.com/notebook/harvard-and-mit-and-captioning/"/>
    <updated>2015-06-29T14:23:28-04:00</updated>
    <id>https://www.aaron-gustafson.com/notebook/harvard-and-mit-and-captioning</id>
    <content type="html"><![CDATA[<p>The U.S. Department of Justice (DOJ) has published Statements of Interest in two cases brought by the National Association of the Deaf (NAD) against <a href="http://www.ada.gov/briefs/harvard_soi.pdf">Harvard (PDF)</a> and <a href="http://www.ada.gov/briefs/mit_soi.pdf">MIT (PDF)</a>, respectively. The NAD is suing the two universities for violations of Title III of the Americans with Disabilities Act (ADA) and Section 504 of the Rehabilitation Act because the video and audio materials they are making available as part of their online learning offerings are not captioned.</p>

<!-- more -->

<p>The DOJ Statements make it quite clear that</p>

<blockquote>
  <p>Both the ADA and Section 504 currently obligate Harvard to provide effective communication to ensure equal access to its online programming services, and resolution of Plaintiffs’ claim involves a straightforward application of longstanding statutory and regulatory requirements. For more than two decades, federal courts have resolved effective communication claims brought under the ADA and Section 504 in a wide range of contexts, including claims alleging unequal access to goods, benefits and services provided through websites or other electronic media. And the Departments of Justice and Education have routinely required covered entities to ensure equal access to goods, benefits and services, electronic or otherwise, through the provision of captioning or other auxiliary aids or services.</p>
</blockquote>

<p>Also…</p>

<blockquote>
  <p>[T]he Department issued an Advanced Notice of Proposed Rulemaking (“ANPRM”) on Accessibility of Web Information and Services of State and Local Government Entities and Public Accommodations, announcing the Department’s interest in developing more specific requirements or technical standards for  website accessibility. … In the ANPRM, the Department reaffirmed its longstanding position that the ADA applies to websites of public accommodations, and reiterated, consistent with the preamble to the 1991 regulations, that the ADA regulations should be interpreted to keep pace with developing technologies.</p>
</blockquote>

<p>Neither case has been settled yet, but the fact that the DOJ is siding with the NAD will lend more credence to their complaints and will likely result in one or both institutions settling out of court and, eventually, captioning their videos.</p>

<p>Captions are critical for the deaf and hard of hearing as they let them know what’s being said, who is saying it, how it’s being said, and inform them of any other sounds that are germane to the content. Reading captions is the equivalent of hearing with your eyes.</p>

<p>Are your videos captioned? If your content is aimed at a broad audience, it might be worth your time (and even your money) to get them captioned.</p>

<p>No doubt, accurate captioning is time-consuming. There are some ways of automating it (<a href="https://support.google.com/youtube/answer/3038280">YouTube does this</a>, for instance), but there are also services like <a href="https://castingwords.com/">Casting Words</a>, <a href="https://www.rev.com/caption">Rev</a>, and <a href="http://www.3playmedia.com/">3Play Media</a> that will do it for a nominal fee.</p>

<p>It’s worth noting that the HTML <code>video</code> element supports subtitles and captions via <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/track">the <code>track</code> element</a> (<a href="http://www.iandevlin.com/blog/2015/04/html5/html5-video-captions-current-browser-status">browser support</a>, <a href="http://caniuse.com/#search=track">Can I Use data</a>). YouTube also allows you to <a href="https://support.google.com/youtube/answer/2734796">add captions and subtitles to your videos manually</a>. But keep in mind that <a href="http://screenfont.ca/learn/">captions and subtitles are not the same thing</a>; captions need to capture more than just the words that are said.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Where Do We Go From Here?]]></title>
    <link href="https://www.aaron-gustafson.com/notebook/where-do-we-go-from-here/"/>
    <updated>2015-06-22T11:49:56-04:00</updated>
    <id>https://www.aaron-gustafson.com/notebook/where-do-we-go-from-here</id>
    <content type="html"><![CDATA[<p><em>I had the great pleasure of delivering the closing keynote for the final Responsive Day Out. Here’s what I had to say.</em></p>

<!-- more -->

<hr />

<p>Today has provided an amazing tour of the world of responsive design. We’ve seen how to level-up our workflows and processes. We’ve learned new ways to improve the accessibility of our products. We’ve grappled with modern CSS and HTML capabilities that help us embrace the hugely variable display sizes that swirl and whirl around us.</p>

<p>We’ve explored the future of modular code and browsers’ capacity for working without network connectivity. And we’ve even taken a trip into the possible future of where the web might go.</p>

<figure id="figure-2015-06-22-01">
<img  src="https://images1-focus-opensocial.googleusercontent.com/gadgets/proxy?url=https://www.aaron-gustafson.com/i/posts/2015-06-22/01.jpg&amp;resize_w=320&amp;container=focus&amp;refresh=2592000 320w" srcset="https://images1-focus-opensocial.googleusercontent.com/gadgets/proxy?url=https://www.aaron-gustafson.com/i/posts/2015-06-22/01.jpg&amp;resize_w=1920&amp;container=focus&amp;refresh=2592000 1920w,https://images1-focus-opensocial.googleusercontent.com/gadgets/proxy?url=https://www.aaron-gustafson.com/i/posts/2015-06-22/01.jpg&amp;resize_w=600&amp;container=focus&amp;refresh=2592000 600w,https://images1-focus-opensocial.googleusercontent.com/gadgets/proxy?url=https://www.aaron-gustafson.com/i/posts/2015-06-22/01.jpg&amp;resize_w=320&amp;container=focus&amp;refresh=2592000 320w" sizes="100vw" alt="">
</figure>

<p>We’ve come a long way since <a href="https://huffduffer.com/adactio/243780">Ethan’s article</a>, fluid grids, flexible media, and media queries. Those three tenets sowed a seed that has grown and flourished as we have come to better understand the implications of device proliferation. We’ve seen that the web is capable of going anywhere and doing pretty much anything.</p>

<p>I would argue that <a href="https://huffduffer.com/adactio/243780">“Responsive Web Design”</a> was the first article that really managed to capture the concepts that John Allsopp had discussed so many years before in <a href="http://www.alistapart.com/articles/dao/">“A Dao of Web Design”</a> and distilled them into something the design and development community could really sink their teeth into. It provided a concrete example of the web’s ability to flex and mold itself into whatever shape it needed to take on.</p>

<p>It was the first time many designers had come to terms with the idea that “experience” was not some monolithic thing.</p>

<p>Sure, many of us in the web standards community had been talking the talk and walking the walk with regard to <a href="http://alistapart.com/article/understandingprogressiveenhancement">progressive enhancement</a>. And we were gaining converts, but progress was slow. Ethan demonstrated—directly and succinctly—what the progressive enhancement of visual design could look like.</p>

<p>Providing an identical experience for each and every human being that tries to access our sites would be impossible. There are simply far too many factors to consider. We’ve got screen size, display density, CPU speed, amount of RAM, sensor availability, feature availability, interface methods … <em>breathe</em> … operating system type, operating system version, browser type, browser version, plug-ins installed, network speed, network latency, network congestion, firewalls, proxies, routers, and probably a dozen other factors my mind is incapable of plucking amid the whirlwind of technical considerations.</p>

<p><strong>And that doesn’t even consider our users.</strong></p>

<p>When it comes to the people we need to reach for our work to actually matter, we have to consider literacy level, reading acumen, level of domain knowledge, cognitive impairments like learning disabilities and dyslexia, attention deficit issues, environmental distractions, vision impairment, hearing impairment, motor impairment, how much they understand how to use their device, how much they understand how to use their browser, how well-versed in common web conventions they are, and a ton of other “human factors”.</p>

<p>Every person is different and everyone comes to the web with their own set of special needs. Some are always with them, blindness for example. Others are transient, like breaking your mousing arm. Still others are purely situational and dependent on the device you are using at the time and its technical capabilities or constraints.</p>

<p>Trying to devise one monolithic experience for each and every person to have in every context that considers every factor would be impossible. And yet, Sir Tim Berners Lee had a vision for a web that was capable of going anywhere. Was he insane?</p>

<p><a href="http://www.w3.org/History/1989/proposal.html">Sir Tim’s vision for the web</a> was that content could be created once and accessed from anywhere. Disparate but related pieces of “hypermedia” scattered across the globe could be connected to one another via links. Moreover, they would be retrievable by anyone on any device capable of reading HTML. For free.</p>

<p><strong>Ultimately, Sir Tim envisioned universal accessibility.</strong></p>

<p>For a great many of us, ensuring our websites are accessible is an afterthought. We talk a good game when it comes to “user centered” this or that, but often treat the word “accessibility” as a synonym for “screen reader”. It’s so much more than that. “Accessibility” is about people. People consume content and use interfaces in many different ways, some similar and some quite dissimilar to how we do it.</p>

<p>Sure, people with visual impairments often use a screen reader to consume content. But they might also use a braille touch feedback device or a braille printer. They probably also use a keyboard. Or they may use a touchscreen in concert with audio cues. Or they may even use a camera to allow them to “read” content via OCR and text-to-speech. And yes, visual impairment affects a decent percentage of the populace (especially as we age), but it is only part of the “accessibility” puzzle.</p>

<p>The contrast between text and the background is an important factor in ensuring content remains readable in different lighting situations. Color choice is an accessibility concern.</p>

<p>The language we use on our sites and in our interfaces directly affects how easy it is for our users to understand what we do, the products we are offering, and why it matters. It also affects how we make our users feel about themselves, their experience, and our companies. Language is an accessibility concern.</p>

<p>The size of our web pages has a direct effect on how long our pages take to download, how much it costs our customers to access them, and (sometimes) even whether or not the content can be reached. Performance is an accessibility concern.</p>

<p>I could keep going, but I’m sure you get the point.</p>

<p>Accessibility is about providing good experiences for everyone, regardless of physical or mental abilities, gender, race, or language. It recognizes that we all have special needs—physical limitations, bandwidth limitations, device limitations—that may require us to  experience the same interface in different ways.</p>

<p>When I visit a website on my phone, for example, I am visually limited by my screen resolution (especially if I am using a browser that encourages zooming) and I am limited in my ability to interact with buttons and links because I am browsing with my fingertips, which are larger and far less accurate than a mouse cursor.</p>

<p>On a touchscreen, I may need the experience to be slightly different, but I still need to be able to do whatever it is I came to the site to do. I need <em>an</em> experience, but moreover I need the <em>appropriate</em> experience.</p>

<p>Embracing the reality that experience does’t need to be just one thing will help us reach more people with fewer headaches. Experience can—and should—be crafted as a continuum. This is progressive enhancement: We start with a baseline experience that works for everyone—content, real links, first generation form controls, and forms that actually submit to the server. Then we build up the experience from there.</p>

<figure id="figure-2015-06-22-02">
<img src="https://www.aaron-gustafson.com/i/posts/2015-06-22/02.gif" alt="" />
</figure>

<p>Your browser supports HTML5 form controls? Great! You’ll get a better virtual keyboard when you go to type your email address. You can use CSS? Awesome, let me make that reading experience better for you. Oh, you can handle media queries! Let me adjust the layout so those line lengths are a little more comfortable. Wow, your browser supports Ajax?! Here let me load in some teasers for related content you might find interesting.</p>

<p>Imagine sitting down in a restaurant only to have the waiter immediately bring you a steak. But you’re a vegetarian. You ask if they offer something you can eat and they politely reply <em>Oh I’m sorry, meat is a requirement. Why don’t you just eat meat? It’s easy! You’re really missing out on some tasty food.</em> No waiter who actually cares about your experience would do that.</p>

<p>And yet we—as an industry—don’t seem to have any problem telling someone they need to change their browser to accommodate us. That’s just wrong. Our work is meaningless without users. We should be bending over backwards to attract and retain them. This is customer service 101.</p>

<p>This comes back to Postel’s law, which Jeremy often recounts:</p>

<blockquote>
  <p>Be conservative in what you do, be liberal in what you accept from others.</p>
</blockquote>

<p>We need to be lax when it comes to browser support and not make to many (or better yet any) assumptions about what we can send.</p>

<p>Of course this is not an approach everyone in our industry is ready to embrace, so I’ll offer another quote I come back to time and time again…</p>

<blockquote>
  <p>When something happens, the only thing in your power is your attitude toward it; you can either accept it or resent it.</p>
</blockquote>

<p>We can’t control the world, we can only control our reaction to it.</p>

<p>Now those of you who’ve gathered for this final Responsive Day Out (or who are following along at home) probably understand this more than most. We feel the constant bombardment of new devices, screen sizes, and capabilities. The only way I’ve found to deal with all of this is to accept it, embrace the diversity, and view device and browser proliferation as a feature, not a bug.</p>

<p>It’s up to us to educate those around us who have—either by accident or intent—not accepted that diversity is the reality we live in and things are only going to get crazier. Burying our heads in the sand is not an option.</p>

<p>When I am trying to help folks understand and embrace diversity, I often reach for one of my favorite thought exercises from <a href="https://en.wikipedia.org/wiki/John_Rawls">John Rawls</a>.</p>

<figure id="figure-2015-06-22-03">
<img  src="https://images1-focus-opensocial.googleusercontent.com/gadgets/proxy?url=https://www.aaron-gustafson.com/i/posts/2015-06-22/03.jpg&amp;resize_w=320&amp;container=focus&amp;refresh=2592000 320w" srcset="https://images1-focus-opensocial.googleusercontent.com/gadgets/proxy?url=https://www.aaron-gustafson.com/i/posts/2015-06-22/03.jpg&amp;resize_w=1920&amp;container=focus&amp;refresh=2592000 1920w,https://images1-focus-opensocial.googleusercontent.com/gadgets/proxy?url=https://www.aaron-gustafson.com/i/posts/2015-06-22/03.jpg&amp;resize_w=600&amp;container=focus&amp;refresh=2592000 600w,https://images1-focus-opensocial.googleusercontent.com/gadgets/proxy?url=https://www.aaron-gustafson.com/i/posts/2015-06-22/03.jpg&amp;resize_w=320&amp;container=focus&amp;refresh=2592000 320w" sizes="100vw" alt="">
</figure>

<p>Rawls was a philosopher who used to run a social experiment with students, church groups, and the like.</p>

<p>In the experiment, participants were allowed to create their ideal society. It could follow any philosophy: It could be a monarchy or democracy or anarchy. It could be capitalist or socialist. The people in this experiment had free rein to control absolutely every facet of the society… but then he’d add the twist: They could not control what position they occupied in that society.</p>

<p>This twist is what <a href="https://en.wikipedia.org/wiki/John_Harsanyi">John Harsanyi</a>—an early game theorist—refers to as the <a href="https://en.wikipedia.org/wiki/Veil_of_ignorance">“Veil of Ignorance”</a> and what Rawls found, time and time again, was that individuals participating in the experiment would gravitate toward creating the most egalitarian societies.</p>

<p>It makes sense: what rational, self-interested human being would treat the elderly, the sick, people of a particular gender, race, creed, or color poorly if they could find themselves in that very same position when the veil is pulled away?</p>

<p>The things we do to accommodate special needs now pay dividends in the future. Look at ramps.</p>

<figure id="figure-2015-06-22-04">
<img  src="https://images1-focus-opensocial.googleusercontent.com/gadgets/proxy?url=https://www.aaron-gustafson.com/i/posts/2015-06-22/04.jpg&amp;resize_w=320&amp;container=focus&amp;refresh=2592000 320w" srcset="https://images1-focus-opensocial.googleusercontent.com/gadgets/proxy?url=https://www.aaron-gustafson.com/i/posts/2015-06-22/04.jpg&amp;resize_w=1920&amp;container=focus&amp;refresh=2592000 1920w,https://images1-focus-opensocial.googleusercontent.com/gadgets/proxy?url=https://www.aaron-gustafson.com/i/posts/2015-06-22/04.jpg&amp;resize_w=600&amp;container=focus&amp;refresh=2592000 600w,https://images1-focus-opensocial.googleusercontent.com/gadgets/proxy?url=https://www.aaron-gustafson.com/i/posts/2015-06-22/04.jpg&amp;resize_w=320&amp;container=focus&amp;refresh=2592000 320w" sizes="100vw" alt="">
</figure>

<p>They’re a classic example of an accessibility feature for people in wheelchairs that also benefit people who aren’t in them: People toting luggage, delivery services hauling heavy things on dollies, parents pushing children (or their dressed up dogs) in strollers, a commuter walking her bike, and that guy who just prefers walking up a gentle incline to expending the effort required to mount a step.</p>

<p>When we create alternative paths to get from Point A to Point B, people can take the one most appropriate for them, whether by choice or necessity. And everyone can accomplish their goals.</p>

<p>We all have special needs. Some we’re born with. Some we develop. Some are temporary. Some have nothing to do with us personally, but are situational or purely dependent on the hardware we are using, the interaction methods we have available to us, or even the speed at which we can access the Internet or process data.</p>

<p>What is responsive web design about if not accessibility? Yes, its fundamental tenets are concerned with visual design, but in terms of the big picture, they’re all about providing the best possible reading experience.</p>

<p>As practitioners of responsive design, we understand the benefits of adapting our interfaces. We understand fallbacks. We understand how to design robust experiences that work under a wide variety of conditions. Every day we broaden the accessibility of our products.</p>

<p>These skills will make us invaluable as technology continues to offer novel ways of consuming and interacting with our websites.</p>

<p>We’re just starting to dip or toes—er, hands—into the world of motion-based gestural controls. Sure, we’ve had them in two dimensions on touch screens for a while now but three dimensional motion-based controls are only beginning to appear.</p>

<p><figure id="fig-VXhhE-l96qQ" class="figure figure--video"><div class="video-embed video-embed--youtube video-embed--16x9"><a class="video-embed__lazy-link" style="background-image:url(https://i2.ytimg.com/vi/VXhhE-l96qQ/0.jpg);" href="//www.youtube.com/watch?v=VXhhE-l96qQ&amp;start=41&amp;end=78" data-lazy-video-src="https://www.youtube.com/embed/VXhhE-l96qQ?autoplay=1&amp;start=41&amp;end=78&amp;modestbranding=1&amp;iv_load_policy=3"><div class="video-embed__lazy-div"></div><div class="video-embed__lazy-info">Kinect Tips, Part 3: Gesture Controls</div></a></div></figure></p>

<p>The first big leap in this direction was <a href="https://en.wikipedia.org/wiki/Kinect">Kinect</a> on the Xbox 360 (and later, Windows). With Kinect, we interact with the computer using body movements like raising a hand (which gets Kinect to pay attention), pushing our hand forward to click/tap, and grasping to drag the canvas in a particular direction.</p>

<p>The Kinect ushered in a major revolution in terms of interfacing with computers, but from an interaction perspective, it presents similar challenges to those of the <a href="https://en.wikipedia.org/wiki/Wii#Wii_Remote">Wii controller</a> and Sony’s <a href="https://en.wikipedia.org/wiki/PlayStation_Move">PlayStation Move</a>. Large body gestures like raising your hand (or a wand controller) can be tiring.</p>

<p><figure id="fig-21LtA5-wiwU" class="figure figure--video"><div class="video-embed video-embed--youtube video-embed--16x9"><a class="video-embed__lazy-link" style="background-image:url(https://i2.ytimg.com/vi/21LtA5-wiwU/0.jpg);" href="//www.youtube.com/watch?v=21LtA5-wiwU&amp;start=7&amp;end=19" data-lazy-video-src="https://www.youtube.com/embed/21LtA5-wiwU?autoplay=1&amp;start=7&amp;end=19&amp;modestbranding=1&amp;iv_load_policy=3"><div class="video-embed__lazy-div"></div><div class="video-embed__lazy-info">Leap Motion With Windows</div></a></div></figure></p>

<p>They’re also not terribly accurate. If you thought that touchscreen accuracy was an issue, hand gestures like those for the Kinect or <a href="https://en.wikipedia.org/wiki/Leap_Motion">LEAP Motion</a> pose even more of a challenge.</p>

<p>To accommodate interactions like this (which we currently have no way of detecting) we need to be aware of how easy it is to click on interactive controls. We need to determine if our buttons and links are large enough and whether there is enough space between them to ensure our user’s intent is accurately conveyed to the browser. Two specs which can help address this are Media Queries Level 4 and Pointer Events.</p>

<p>In <a href="http://dev.w3.org/csswg/mediaqueries-4/">Media Queries Level 4</a>, we became able to apply style rules to particular interaction contexts. For instance, when we have very accurate control over our cursor (as in the case of a stylus or mouse) or less accurate control (as in the case of a touch screen or physical gesture):</p>

<p><div><script src='https://gist.github.com/372271534c78cf11d4a6.js?file=mq4-pointer.css'></script>
<noscript><pre><code>@media (pointer:fine) {
  /* Smaller links and buttons are ok */
}
@media (pointer:coarse) {
  /* Larger links and buttons are probably a good idea */
}</code></pre></noscript></div>
</p>

<p>Of course, we want to offer a sensible default in terms of size and spacing as a fallback for older browsers and devices.</p>

<p>We also have the ability to determine whether the device is capable of hovering over an element and can adjust the interface accordingly.</p>

<p><div><script src='https://gist.github.com/372271534c78cf11d4a6.js?file=mq4-hover.css'></script>
<noscript><pre><code>@media (hover:hover) {
  /* hover-related interactions are A-OK */
}
@media (hover:on-demand) {
  /* hover-related interactions are potentially difficult,
     maybe do something else instead */
}
@media (hover:none) {
	/* No hover possible :-( */
}</code></pre></noscript></div>
</p>

<p>We still need to figure out how well all of this ends up working on multimodal devices like the Surface tablet, however. Will the design change as the user switches between input modes? Should it? To that end, the spec also provides <code>any-pointer</code> and <code>any-hover</code> to allow you to query for whether <em>any</em> supported interaction method meets your requirements, but here’s a word of warning from the spec:</p>

<blockquote>
  <p>Designing a page that relies on hovering or accurate pointing only because <code>any-hover</code> or <code>any-pointer</code> indicate that an input mechanism with these capabilities is available, is likely to result in a poor experience.</p>
</blockquote>

<p>These media query options are starting to roll out in Chrome, Mobile Safari, and Microsoft Edge, so it’s worth taking a look at them.</p>

<p><a href="http://www.w3.org/TR/pointerevents/">Pointer Events</a> is another spec that is beginning to gain some traction. It generalizes interaction to a single event rather than forcing us to silo experience into mouse-driven, touch-driven, pen-driven, (sigh) force-driven, and so on.</p>

<p>We can unobtrusively detect support for Pointer Events…</p>

<p><div><script src='https://gist.github.com/372271534c78cf11d4a6.js?file=pointer-test.js'></script>
<noscript><pre><code>if ( window.PointerEvent ) {
  window.addEventListener( &quot;pointerdown&quot;, detectType, false );
}</code></pre></noscript></div>
</p>

<p>…and then handle them all in the same way or create branches based on the <code>pointerType</code>:</p>

<p><div><script src='https://gist.github.com/372271534c78cf11d4a6.js?file=pointer-event.js'></script>
<noscript><pre><code>function detectType( event ) {
  switch( event.pointerType ) {
    case &quot;mouse&quot;:
      /* mouse input detected */
      break;
    case &quot;pen&quot;:
      /* pen/stylus input detected */
      break;
    case &quot;touch&quot;:
      /* touch input detected */
      break;
    default:
      /* pointerType is empty (could not be detected) or UA-specific custom type */
  }
}</code></pre></noscript></div>
</p>

<p>Of course, in addition to considering the level of accuracy our users have while interacting with our screens, we also need to consider the potentially increased distance at which our users are reading our content.</p>

<p>To that end, I’ve been experimenting with the viewport width (<code>vw</code>) unit.</p>

<p>For a long time, I’ve used ems for the layout’s <code>max-width</code> (so the line length is proportional to the font size). I also use relative font sizes. With that as the foundation, I can use a media query that matches the maximum width and set the base font size at the vw equivalent at the max width.</p>

<p><div><script src='https://gist.github.com/372271534c78cf11d4a6.js?file=vw-scaling.css'></script>
<noscript><pre><code>body {
  max-width: 64em;
}

@media screen and (min-width: 64em) {
  body {
    font-size: 1.5625vw; /* ( 1em / 64em ) * 100 */
  }
}</code></pre></noscript></div>
</p>

<p>Then the whole design will simply zoom the layout when viewed beyond that size.</p>

<p><figure id="fig-6XoN9mMgI38" class="figure figure--video"><div class="video-embed video-embed--youtube video-embed--16x9"><a class="video-embed__lazy-link" style="background-image:url(https://i2.ytimg.com/vi/6XoN9mMgI38/0.jpg);" href="//www.youtube.com/watch?v=6XoN9mMgI38" data-lazy-video-src="https://www.youtube.com/embed/6XoN9mMgI38?autoplay=1&amp;modestbranding=1&amp;iv_load_policy=3"><div class="video-embed__lazy-div"></div><div class="video-embed__lazy-info">Using CSS to Scale a Design on Resize</div></a></div></figure></p>

<p>If you don’t want to turn something like that on automatically, you can enable it to be toggled on and off with JavaScript.</p>

<p><figure id="fig-96l_W7ca6SM" class="figure figure--video"><div class="video-embed video-embed--youtube video-embed--16x9"><a class="video-embed__lazy-link" style="background-image:url(https://i2.ytimg.com/vi/96l_W7ca6SM/0.jpg);" href="//www.youtube.com/watch?v=96l_W7ca6SM" data-lazy-video-src="https://www.youtube.com/embed/96l_W7ca6SM?autoplay=1&amp;modestbranding=1&amp;iv_load_policy=3"><div class="video-embed__lazy-div"></div><div class="video-embed__lazy-info">CSS Couch Mode</div></a></div></figure></p>

<p>Things get even crazier when you start to factor in devices like the HoloLens. And no, I have not gotten to play with one yet.</p>

<p><figure id="fig-3AADEqLIALk" class="figure figure--video"><div class="video-embed video-embed--youtube video-embed--16x9"><a class="video-embed__lazy-link" style="background-image:url(https://i2.ytimg.com/vi/3AADEqLIALk/0.jpg);" href="//www.youtube.com/watch?v=3AADEqLIALk&amp;start=87&amp;end=117" data-lazy-video-src="https://www.youtube.com/embed/3AADEqLIALk?autoplay=1&amp;start=87&amp;end=117&amp;modestbranding=1&amp;iv_load_policy=3"><div class="video-embed__lazy-div"></div><div class="video-embed__lazy-info">Microsoft HoloLens demo onstage at BUILD 2015</div></a></div></figure></p>

<p>But the idea of being able to drop a resizable virtual screen on any surface presents some interesting possibilities as a user and some unique challenges as a designer. HoloLens, of course, brings with it gesture controls as well, so accounting for a variety of input types should get us pretty far.</p>

<p>In a similar vein, we should begin to think about what experiences can and should look like when we are browsing solely with our gaze. Gaze tracking has its origins in the accessibility space as a means of providing interface control to folks with limited or no use of their hands. Traditionally, gaze tracking hardware has been several thousand dollars, putting it out of the reach of many people, but that is starting to change.</p>

<p>In the last few years, the computational power of our devices has increased as the hardware costs associated with supporting gaze tracking have dropped dramatically. Looking around, you can see gaze tracking beginning to move into the public sphere: Many smartphones and smartwatches can recognize when you are looking at them (or at least they do sometimes). This is only a short step away from knowing where on the screen you are looking. And nearly every high-end smartphone is now equipped with a front-facing camera which makes them perfect candidates to provide this interaction method.</p>

<p><figure id="fig-DEk7PlJWQgI" class="figure figure--video"><div class="video-embed video-embed--youtube video-embed--16x9"><a class="video-embed__lazy-link" style="background-image:url(https://i2.ytimg.com/vi/DEk7PlJWQgI/0.jpg);" href="//www.youtube.com/watch?v=DEk7PlJWQgI&amp;start=18&amp;end=54" data-lazy-video-src="https://www.youtube.com/embed/DEk7PlJWQgI?autoplay=1&amp;start=18&amp;end=54&amp;modestbranding=1&amp;iv_load_policy=3"><div class="video-embed__lazy-div"></div><div class="video-embed__lazy-info">The Sesame Phone</div></a></div></figure></p>

<p>The <a href="http://sesame-enable.com/phone/">Sesame Phone</a> was designed to allow people to use a smartphone without using their hands. It uses facial tracking to move a virtual cursor around the screen, allowing users to interact with the underlying operating system as well as individual applications. It supports tap, swipe, and other gestures (via a context menu) and is pretty impressive in my experience. Technology like this enables people suffering from MS, arthritis, Muscular Dystrophy, and more to use a smartphone and—more importantly to us—browse the web.</p>

<p><a href="https://theeyetribe.com/">The Eye Tribe</a> and <a href="http://www.fixational.com/">Fixational</a> are similarly working to bring eye tracking to smartphones and tablets. Eye tracking is similar to face tracking, but the cursor follows your focus. Micro gestures—blink, wink, etc.—allow you to interact with the device.</p>

<p>Even though most gaze tracking software mimics a mouse and has adjustable sensitivity, the accuracy of it as a pointer device is not fantastic. When I’ve used the Sesame Phone, for instance, I’ve have a hard time controlling the position of my head in order to hold the cursor still to hover and click a button. I’m sure this would improve with practice, but it’s safe to say that in a gaze interaction, larger, well spaced, and more easily targeted links and buttons would be a godsend.</p>

<p>So far, I’ve focused on interaction methods that facilitate navigation and consuming content. But what about filling out a form? I can tell you that typing an email letter-by-letter on a virtual keyboard using your face, sucks…</p>

<p>Thankfully, most of these gestural implementations are coupled with some form of voice recognition. The Kinect, for instance, will accept verbal commands to navigate and accomplish tasks like filling in forms. The Sesame Phone also supports voice commands for certain basic actions, dictating email, and the like.</p>

<p>Coupled with voice, the alternative interaction methods of Kinect and Sesame Phone work really well. But voice interaction can stand on its own too.</p>

<p>Most of us are familiar with <a href="https://en.wikipedia.org/wiki/Siri">Apple’s Siri</a>, <a href="https://en.wikipedia.org/wiki/Google_Now">Google Now</a>, and <a href="https://en.wikipedia.org/wiki/Microsoft_Cortana">Microsoft’s Cortana</a>. These digital assistants are great at retrieving information from select sources and doing other assistant-y things like calculating a tip and setting a reminder. As far as interacting with the web, however, they don’t… yet. We can engage with them, but they can‘t (necessarily) engage with a web page.</p>

<p>Exposing the information stored in our webpages via semantic HTML and structured syntaxes like <a href="http://microformats.org/">microformats</a>, <a href="https://en.wikipedia.org/wiki/Microdata_(HTML)">microdata</a>, and <a href="https://en.wikipedia.org/wiki/RDFa">RDFa</a> <em>should</em> eventually make our content available to these assistants, but we don’t really know. Their various makers haven’t really given us any clue as to how to do that and, as it stands right now, none of them can look up a web page and read it to you. For that you need to invoke a screen reader.</p>

<p>Each company offers a first-party screen reader. And all are capable of helping you interact with a page, including helping you fill in forms, without having to see the page. And yet, these technologies have not been coupled with their corresponding assistants. It probably won’t be long before we see that happen.</p>

<p>When we start to consider how our websites will be experienced in a voice context, the readability of our web pages becomes crucial. Clear well-written prose and a logical source order will be an absolute necessity. If our pages don’t make sense when read, what’s the point?</p>

<p>Content strategist Steph Hay views interface as an opportunity to have a conversation with our users. Soon it literally will be.</p>

<p>Interestingly, Microsoft has given us a peek at what it might be like to design custom voice commands for our websites beyond what the OS natively supports with Cortana. In other words, they let us teach their assistant.</p>

<p>In Windows 10, installable web apps can include a <a href="https://msdn.microsoft.com/en-us/library/windows/apps/dn722331.aspx">Voice Command Definition (VCD) file</a> in the <code>head</code> of the page to enable custom commands:</p>

<p><div><script src='https://gist.github.com/372271534c78cf11d4a6.js?file=vcd.html'></script>
<noscript><pre><code>&lt;meta name=“msapplication-cortanavcd&quot; content=&quot;http://myapp.io/vcd.xml&quot;&gt;</code></pre></noscript></div>
</p>

<p>The referenced VCD file is simply an XML file defining the keyword for the web app and commands that can be issued.</p>

<p>Using very basic syntax, The VCD identifies optional pieces of a given phrase and variables Cortana should extract:</p>

<p><div><script src='https://gist.github.com/372271534c78cf11d4a6.js?file=vcd.xml'></script>
<noscript><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;
&lt;VoiceCommands xmlns=&quot;http://schemas.microsoft.com/voicecommands/1.2&quot;&gt;
  &lt;CommandSet xml:lang=&quot;en-us&quot; Name=&quot;groupPost&quot;&gt;
    &lt;CommandPrefix&gt;Group Post&lt;/CommandPrefix&gt;
    &lt;Example&gt;Group Post add note&lt;/Example&gt;
    &lt;Command Name=&quot;addNote&quot;&gt;
      &lt;Example&gt;add a note {message} using group post&lt;/Example&gt;
      &lt;ListenFor RequireAppName=&quot;BeforeOrAfterPhrase&quot;&gt;[please] add a note [that] {noteSubject}&lt;/ListenFor&gt;
      &lt;Feedback&gt;adding {noteSubject} to Group Post&lt;/Feedback&gt;
      &lt;Navigate Target=&quot;/addNote.htm&quot;/&gt;
    &lt;/Command&gt;
    &lt;PhraseTopic Label=&quot;noteSubject&quot; Scenario=&quot;Dictation&quot;&gt;&lt;/PhraseTopic&gt;
  &lt;/CommandSet&gt;
&lt;/VoiceCommands&gt;</code></pre></noscript></div>
</p>

<p>This particular app passes the captured information over to JavaScript for processing. That’s right, <a href="https://msdn.microsoft.com/en-us/library/dn722330.aspx#handle_activation_and_execute_voice_commands">Cortana has a JavaScript API too</a>. Pretty neat.</p>

<p>But traditional computers and smart mobile devices aren’t the only place we’re starting to see voice based experiences. We also have disembodied voices like <a href="https://en.wikipedia.org/wiki/Amazon_Echo">Amazon’s Echo</a> and <a href="http://www.theubi.com/">the Ubi</a> which are completely headless.</p>

<figure id="figure-2015-06-22-11">
<img  src="https://images1-focus-opensocial.googleusercontent.com/gadgets/proxy?url=https://www.aaron-gustafson.com/i/posts/2015-06-22/05.jpg&amp;resize_w=320&amp;container=focus&amp;refresh=2592000 320w" srcset="https://images1-focus-opensocial.googleusercontent.com/gadgets/proxy?url=https://www.aaron-gustafson.com/i/posts/2015-06-22/05.jpg&amp;resize_w=1920&amp;container=focus&amp;refresh=2592000 1920w,https://images1-focus-opensocial.googleusercontent.com/gadgets/proxy?url=https://www.aaron-gustafson.com/i/posts/2015-06-22/05.jpg&amp;resize_w=600&amp;container=focus&amp;refresh=2592000 600w,https://images1-focus-opensocial.googleusercontent.com/gadgets/proxy?url=https://www.aaron-gustafson.com/i/posts/2015-06-22/05.jpg&amp;resize_w=320&amp;container=focus&amp;refresh=2592000 320w" sizes="100vw" alt="">
</figure>

<p>Right now, they both seem squarely focused on helping your house become “smarter”—streaming music, adjusting the thermostat, etc.—but it isn’t hard to imagine these devices becoming coupled with the ability to browse and interact with the web.</p>

<p>In the near future, voice-based interactions with the web will be entirely possible. They will likely suck a bit at first, but they’ll get better.</p>

<p>I’m going to make a somewhat bold prediction: while touch has been revolutionary in many ways toward improving digital access, voice is going to be even more significant. Voice-based interfaces will create new opportunities for people to interact with and participate in the digital world.</p>

<p>Because we’ve been thinking about how the experiences we create are consumable across a variety of devices, we’ve got the jump on other folks working on the web when it comes to voice. We see experience as a continuum, starting with text.</p>

<p>As voice technology matures, we will be the ones people look to as the experts. We will empower the next generation of websites and applications to become voice-enabled and in so doing, we will improve the lives of billions. Because “accessibility” is not about disabilities, it’s about access and <strong>it’s about people</strong>.</p>

<p>Sure, we’ll make it easier to look up movie times and purchase tickets to see the latest <cite>Transformers</cite> debacle, but we will also empower the nearly 900 million people globally—over 60% of whom are women—that are illiterate. And that’s a population that has been largely ignored on our dominantly textual web.</p>

<p>We will create new opportunities for the poor and disadvantaged to participate in a world that has excluded them. You may not be aware, but 80% of Fortune 500 companies—think Target, Walmart—only accept job applications online or via computers. We will enable people who have limited computer skills or who struggle with reading to apply for jobs with these companies.</p>

<p>We can help bridge the digital divide and the literacy gap. We can create opportunities for people to better their lives and the lives of their families. We have the power to create more equity in this world than most of us have ever dreamed.</p>

<p>This is an incredibly exciting time, not just for the responsive design community, not just the web, but for the world! The future is coming and I can’t wait to see how awesome you make it!</p>

<figure id="figure-2015-06-22-12">
<img  src="https://images1-focus-opensocial.googleusercontent.com/gadgets/proxy?url=https://www.aaron-gustafson.com/i/posts/2015-06-22/06.jpg&amp;resize_w=320&amp;container=focus&amp;refresh=2592000 320w" srcset="https://images1-focus-opensocial.googleusercontent.com/gadgets/proxy?url=https://www.aaron-gustafson.com/i/posts/2015-06-22/06.jpg&amp;resize_w=1920&amp;container=focus&amp;refresh=2592000 1920w,https://images1-focus-opensocial.googleusercontent.com/gadgets/proxy?url=https://www.aaron-gustafson.com/i/posts/2015-06-22/06.jpg&amp;resize_w=600&amp;container=focus&amp;refresh=2592000 600w,https://images1-focus-opensocial.googleusercontent.com/gadgets/proxy?url=https://www.aaron-gustafson.com/i/posts/2015-06-22/06.jpg&amp;resize_w=320&amp;container=focus&amp;refresh=2592000 320w" sizes="100vw" alt="">
</figure>

<hr />

<p><em>Responsive Day Out 3: The Final Breakpoint was held in Brighton, UK on 19 June 2015.</em></p>

<ul>
  <li><a href="https://huffduffer.com/adactio/243780">Listen to this presentation on Huffduffer</a>.</li>
  <li>Read <a href="https://decadecity.net/blog/2015/06/19/aaron-gustafson-where-do-we-go-here">Orde Saunders’ notes</a> from my talk.</li>
  <li>Read <a href="https://hiddedevries.nl/en/blog/2015-06-20-responsive-day-out-3-the-final-breakpoint/">Hidde de Vries’ recap of the day</a>.</li>
</ul>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Consider How Your Forms Read]]></title>
    <link href="https://www.aaron-gustafson.com/notebook/consider-how-your-forms-read/"/>
    <updated>2015-02-23T11:21:24-05:00</updated>
    <id>https://www.aaron-gustafson.com/notebook/consider-how-your-forms-read</id>
    <content type="html"><![CDATA[<p>While listening to <a href="http://www.radiolab.org/story/trust-engineers/">Radiolab’s “The Trust Engineers”</a>, I’ll admit I got a little excited when they started talking about web form performance. And no, not “performance” in the time-to-download sense, but “performance” in terms of how well the forms performed in attempting to capture meaningful, actionable data.</p>

<!-- more -->

<p>I’ll set the scene: It’s the holiday season in 2011 and people are uploading photos to Facebook like crazy. In the span of a few days, Facebook processed more photo uploads than are contained in the entirety of Flickr. Seriously, that’s a lot of photos.</p>

<p>Anyway, one unintended consequence of this deluge of photo uploads was a significant uptick in <a href="https://www.facebook.com/help/189722821075378">photo reports</a>. Facebook received millions of them, but they made no sense: Moms holding babies reported for harassment, pictures of puppies reported for hate speech, and so on. Roughly 97% of these photo reports were miscategorized.</p>

<p>When Facebook engineers reached out to some of the users who had reported these photos for a bit more background behind their decisions, they discovered that many of the reports were because users didn’t want the photo on Facebook reasons other than the options provided. In some cases it was because they didn’t like how they looked in the photo and in others it was because the photo was of an ex or even a beloved pet they shared with an ex.</p>

<p>The existing photo reporting form had not done a good job of accounting for these more personal reasons for wanting a photo removed, so the Facebook engineers went to work. They added a step that asked <em>How does this photo make you feel?</em> The options were simple:</p>

<ul>
  <li>Embarrassing</li>
  <li>Upsetting</li>
  <li>Saddening</li>
  <li>Bad Photo</li>
  <li>Other</li>
</ul>

<p>The “other” option also provided a free-response field to fill in.</p>

<p>With this system in place, they found that 50% of reporters would choose one of the provided options. That was pretty helpful, but there was still a problem: 34% of the “other” respondents were writing “It’s embarrassing” in the blank rather than choosing the “embarrassing” option already provided.</p>

<p>What they realized was that people were not identifying with the “embarrassing” text (or may have even thought it was referring to them, rather than assuming the implied “It’s”). A subtle shift in language was needed, so they changed the label to <em>Please describe the photo</em>. And they updated the options to mirror how people actually talk:</p>

<ul>
  <li>It’s embarrassing</li>
  <li>It’s a bad photo of me</li>
  <li>It makes me sad</li>
</ul>

<p>With this subtle change, they were able to increase the percentage of photo reporters who chose one of the options provided to a whopping 78%.</p>

<p>Sometimes we feel compelled to create forms that are very clinical. In the survey world we’re often taught to limit the “personality” of our forms in order to avoid influencing the responses.</p>

<p>That said, we need to remember that we are authoring interfaces that will be used by actual people. When we are creating forms that don’t require that kind of scientific rigor, we can (and should) do whatever we can to make the interaction more human.</p>

<p>Ask real questions: <em>What’s your name?</em>, <em>What’s your email?</em>, and <em>How would you prefer we contact you?</em> are far more friendly than <em>Name</em>, <em>Email</em>, and <em>Contact Preference</em>.</p>

<p>A little thoughtful consideration for the people who need to fill in your form goes a long way toward making them feel at ease and also helps to ensure the feedback you receive is accurate, valuable, and actionable.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Egalitarianism]]></title>
    <link href="https://www.aaron-gustafson.com/notebook/egalitarianism/"/>
    <updated>2015-01-19T10:32:20-05:00</updated>
    <id>https://www.aaron-gustafson.com/notebook/egalitarianism</id>
    <content type="html"><![CDATA[<p>Today is Martin Luther King Day in the United States, so I thought I’d take a moment to reflect on one aspect of equality I think is incredibly important: egalitarianism.</p>

<!-- more -->

<p>According to <a href="http://www.merriam-webster.com/dictionary/egalitarianism"><cite class="book">Merriam Webster</cite></a>, egalitarianism is:</p>

<blockquote>
	<ol>
		<li>a belief in human equality especially with respect to social, political, and economic affairs;</li>
		<li>a social philosophy advocating the removal of inequalities among people.</li>
	</ol>
</blockquote>

<p>It’s a simple philosophy inspired by <a href="http://en.wikipedia.org/wiki/Golden_Rule">the Golden Rule</a>, an ethical code which is central to most major religions:</p>

<dl>
	<dt>Buddhism</dt>
	<dd>Hurt not others in ways that you yourself would find harmful.</dd>
	<dt>Christianity</dt>
	<dd>Do unto others as you would have  them do unto you.</dd>
	<dt>Islam</dt>
	<dd>No one of you is a believer until he desires for his brother that which he desires for himself.</dd>
	<dt>Judaism</dt>
	<dd>What is hateful to you, do not to your fellow man. That is the entire law; all the rest is commentary.</dd>
</dl>

<p>Heck, even Confucius said “Never impose on others what you would not choose for yourself”.</p>

<p>With so much emphasis on treating others with the sort of respect that we would like to be given, you’d think that inequality would be a non-issue. Obviously that’s not the case.</p>

<p>For centuries, we humans—even those of us who ascribe to <a href="http://en.wikipedia.org/wiki/Golden_Rule#Religion_and_philosophy">these and the other countless Golden Rule abiding religions and philosophies</a>—have failed to recognize ourselves in others and have erected barriers (both physical and societal) to their ability to lead the sort of happy, fulfilled life that we want for ourselves and our families.</p>

<p>Almost every way we mistreat others—from rude or snarky comments to genocide—stems from our inability to empathize with another person or group of people. It’s hard to connect with people who are different than us—people who have different life experiences, people who have different perspectives, people who are challenged in ways we have never been—and when we struggle to create a connection, it becomes easy for us to view them as “the other”. And when we begin to look at other people this way, we lose sight of their humanity and we lose sight of all of the things that make us similar.</p>

<p>While it is completely true that I have a very different life than a woman growing up in <a href="http://en.wikipedia.org/wiki/Dharavi">Dharavi</a>, I have to believe that we have a lot in common too. We both want a good life for our families. We both want to feel safe. We both crack jokes. We’re both human. Our respective societies may view us very differently, but she is no less valuable than I am.</p>

<p>And this is why I am so passionate about the philosophy of egalitarianism. It grounds us in the notion that we are all equally valuable and should be granted equal opportunity to all that life has to offer.</p>

<hr />

<p>John Rawls has a great thought experiment that I often cite in <a href="http://vimeo.com/70018634">my talks on empathy</a>. In it, he would ask you to imagine your ideal society. It could be a monarchy, anarchy, capitalist, or communist. It could be ruled by people of one particular gender or color. It could be governed by people of a particular tribe. It could be <a href="http://en.wikipedia.org/wiki/The_Hunger_Games_universe#Panem">Panem</a>. It could be <a href="http://en.wikipedia.org/wiki/Erewhon">Erewhon</a>. It could be <a href="http://en.wikipedia.org/wiki/Brobdingnag">Brobdingnag</a>. The choice is yours.</p>

<p>Once you’ve been ruminating on this a bit, he drops the bombshell: You have no control over or knowledge of where you fit in this society. This is called the <a href="http://en.wikipedia.org/wiki/Veil_of_ignorance">Veil of Ignorance</a>, a creation of <a href="http://en.wikipedia.org/wiki/John_Harsanyi">John Harsanyi</a> (an economist and early father of <a href="http://en.wikipedia.org/wiki/Game_theory">game theory</a>). Rawls found that, with the Veil of Ignorance in play, people who participated in this thought experiment gravitated toward creating the most egalitarian societies possible.</p>

<p>It makes complete sense: What rational human being would create a society that enslaves people if they themselves could turn out to be a slave? Who would create a society that excludes women if they might be female? Who would build a world full of stairs if they could be in a wheelchair?</p>

<p>I love this exercise because it makes it easy to create at least basic connections between you and a wide variety of people who are different than you. It helps you realize that we are all equal and all worthy of consideration.</p>

<hr />

<p>What is interesting about egalitarianism, as opposed to similar sounding philosophies like <a href="http://en.wikipedia.org/wiki/The_Communist_Manifesto">modern communism</a>, is that it recognizes that equality of opportunity does not necessitate equality of outcome. In other words, while pushing for equality,  it simultaneously recognizes that we have differences in capability, capacity, and interest.</p>

<p>We are different. We are similar in more ways than we are different, but to ignore our differences is to deny us our  individuality, our personhood, our true selves. But recognizing differences is not the same as assigning a value to those differences. That’s an incredibly important distinction and bears repeating: Just because you recognize that someone is different does not imply you should view or treat them as any more or less human on account of what makes them different. That said, recognizing differences is the first step towards being able to create equality of opportunity.</p>

<p>This is such an important concept in life, but I was not a philosophy major, so I’ll stick to discussing it in terms of what I know a lot about: The Web. Specifically, Web accessibility.</p>

<p>For a great many of us, ensuring our websites are accessible is an afterthought. We talk a good game when it comes to “user centered” this or that, but often treat the word “accessibility” as a synonym for “screen reader”. It’s so much more than that. “Accessibility” is about people. People consume content and use interfaces in many different ways, some similar and some not so similar to how we do it.</p>

<p>Sure, people with visual impairments often use a screen reader to consume content. They might also use a braille touch feedback device or a braille printer. They probably also use a keyboard. Or they may use a touchscreen in concert with audio cues. And yes, visual impairment affects a great percentage of people, but they are only part of the “accessibility” puzzle.</p>

<p>The dimensions of interactive elements—links, buttons, etc.—and their proximity to one another is an important factor in ensuring an interface actually registers our intent (i.e. it helps us avoid <a href="http://en.wikipedia.org/wiki/Typographical_error">fat fingering</a>). <em>Design</em> is an accessibility concern.</p>

<p>The color contrast between text and the background is an important factor in ensuring content remains readable in different lighting situations. <em>Color</em> is an accessibility concern.</p>

<p>The language we use on our sites and in our interfaces directly affects how easy it is for our users to understand what we do, the products we are offering, and why it matters. It also affects how we make our users feel. <em>Language</em> is an accessibility concern.</p>

<p>The size of our Web pages and their associated assets has a direct affect on how long our pages take to download, how much it costs our customers to access them, and (sometimes) even whether or not the content can be reached. <em>Performance</em> is an accessibility concern.</p>

<p>I could keep going, but I’m sure you’re starting to get the point. “Accessibility” is ultimately about ensuring people have equal opportunity to access your content while simultaneously recognizing that we all have special needs—physical limitations, bandwidth limitations, device limitations, etc.—that lead us to have different experiences of the same Web page.</p>

<p>Accessibility <em>is</em> egalitarianism.</p>

<hr />

<p>We are all different, but we all human. We all deserve respect. We all deserve to be treated equally. To be treated fairly.</p>

<p>We need more egalitarianism in this world and the good news is that <em>we</em> can make it happen.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The Web Is for Everyone]]></title>
    <link href="https://www.aaron-gustafson.com/notebook/the-web-is-for-everyone/"/>
    <updated>2014-11-06T15:47:54-05:00</updated>
    <id>https://www.aaron-gustafson.com/notebook/the-web-is-for-everyone</id>
    <content type="html"><![CDATA[<p><em>I gave this speech as the closing keynote at <a href="http://a11yqc.org/">A11yQC</a>, a conference on Web accessibility, on 14 October 2014 in Québec City, Canada. I have published my script here as the slides can’t really convey its message on their own.</em></p>

<p>We, as an industry, tend to have a pretty myopic view of experience. Those of us who work day-to-day in accessibility probably have a broader perspective than most, but I would argue that even we all fall short now and again when it comes to seeing the Web as others do.</p>

<!-- more -->

<h2 id="i-we-are-surrounded-by-technology">I: We are surrounded by technology</h2>

<p>We live in a bubble. We are surrounded by technology. Most of us grew up on desktops and laptops. Most of us carry at least one device on us at all times. For some of us, it’s a smartphone of some sort. For others it might be some sort of wearable—a fitness tracker or smart watch. And I’m willing to bet that a large number of us here in this auditorium may even have three or more devices on us at this very moment. I know I do.</p>

<p>And our technology is some of the newest, fastest, and most fully-featured in the world. Our devices are incredibly powerful and make us even more powerful, enabling us to tackle a wide range of tasks with ease. Most of the smartphones we carry come standard with pretty impressive assistive technology built in too, from adjustable text sizes to voice assist and screen reading tools to haptic, and other forms of feedback.</p>

<p>And they are brimming with sensors that extend our natural abilities: GPS, cameras, accelerometers. If you’re blind, your smartphone can help you pick out a matching outfit by identifying complementary colors. It can tell you who is standing in front of you by running facial recognition software. It can help you take a photograph of a document and then read it to you.</p>

<p>This is amazing stuff.</p>

<p>And, if headlines are to be believed, the smartphone revolution is spreading like wildfire. It seems nearly every other week there is some new report about how smartphone sales are continuing to soar. Heck, no one even seems to mention the humble feature phone anymore. And if you keep up with the tech press, CPUs, GPUs, operating systems and browsers keep getting faster and faster and JavaScript is the savior of us all.</p>

<p>The sky is the limit!</p>

<p>Beyond the devices we carry with us everywhere, more and more of our homes are being assimilated into the Borg of the Internet through smart appliances and fixtures like Nest. Tools like these make it easier to control our homes (and our budgets). They empower previously dependent people to live more independent lives.</p>

<p>And of course there’s the coolness factor of being able to turn on your heat while on your way home from work. These advancements are incredible!</p>

<figure id="fig-2014-11-06-01" class="media-container">
	<img src="https://www.aaron-gustafson.com/i/posts/2014-11-06/01-sm.jpg" srcset="/i/posts/2014-11-06/01-lg.jpg 1920w, /i/posts/2014-11-06/01-md.jpg 600w, /i/posts/2014-11-06/01-sm.jpg 320w" alt="Two pie charts comparing Internet usage in the U.S. and Canada." />
</figure>

<p>Of course, what enables all of these things to be as amazing as they are is our ubiquitous connectivity. According to <a href="http://www.InternetLiveStats.com">Internet Live Stats</a>, roughly 86% of Americans use the Internet. You Canadians are a wee bit more “online” at 93%.</p>

<figure id="fig-2014-11-06-02" class="media-container">
	<img src="https://www.aaron-gustafson.com/i/posts/2014-11-06/02-sm.jpg" srcset="/i/posts/2014-11-06/02-lg.jpg 1920w, /i/posts/2014-11-06/02-md.jpg 600w, /i/posts/2014-11-06/02-sm.jpg 320w" alt="Two pie charts comparing the number of mobile data connections in the U.S. and Canada." />
</figure>

<p>America does have you beat when it comes to mobile connectivity: there are over 100 mobile data subscriptions per 100 individuals in the U.S. (probably because of the whole multi-device thing). Mobile connections in Canada are around 53 per 100 people. <small><a href="http://www.oecd.org/sti/broadband/oecdbroadbandportal.htm">(source)</a></small></p>

<figure id="fig-2014-11-06-03" class="media-container">
	<img src="https://www.aaron-gustafson.com/i/posts/2014-11-06/03-sm.jpg" srcset="/i/posts/2014-11-06/03-lg.jpg 1920w, /i/posts/2014-11-06/03-md.jpg 600w, /i/posts/2014-11-06/03-sm.jpg 320w" alt="Two pie charts comparing wired Internet connections in the U.S. and Canada." />
</figure>

<p>Wired connectivity is a bit lower: 30 for every 100 in the U.S. and 33 for every 100 in Canada. <small><a href="http://www.oecd.org/sti/broadband/oecdbroadbandportal.htm">(source)</a></small></p>

<figure id="fig-2014-11-06-04" class="media-container">
	<img src="https://www.aaron-gustafson.com/i/posts/2014-11-06/04-sm.jpg" srcset="/i/posts/2014-11-06/04-lg.jpg 1920w, /i/posts/2014-11-06/04-md.jpg 600w, /i/posts/2014-11-06/04-sm.jpg 320w" alt="A bar chart depicting the cost range, per megabit per month, in the U.S. and Canada." />
</figure>

<p>Connectivity is relatively cheap for you Canadians as well. You pay somewhere in the neighborhood of 39¢-$9.86 per megabyte per month. In the U.S., fees range widely from 53¢ to a whopping $41.70 per megabyte per month. <small><a href="http://www.oecd.org/sti/broadband/oecdbroadbandportal.htm">(source)</a></small></p>

<p>Comcast and Rogers may be equally hated on our respective sides of the 49th parallel, but Comcast clearly sucks just a little bit more. (America!)</p>

<p>This technology and access makes it possible for us to live richer lives and post photos of our cats and kids on Instagram, but it has insulated us. We live in a fantasy world of speed, high definition, and Beats by Dre. Sadly, our experience is far from the reality most of the world lives in.</p>

<figure id="fig-2014-11-06-05" class="media-container">
	<img src="https://www.aaron-gustafson.com/i/posts/2014-11-06/05-sm.jpg" srcset="/i/posts/2014-11-06/05-lg.jpg 1920w, /i/posts/2014-11-06/05-md.jpg 600w, /i/posts/2014-11-06/05-sm.jpg 320w" alt="A bar chart comparing the average monthly incomes of people living in the U.S., Canada, China, and India." />
</figure>

<p>The average American takes home $3,263 a month. For the average Canadian, that figure is $2,724. By comparison, the average worker in China makes $656 a month. But that is a fortune compared to folks in India, who only take home $295 for a month of hard work. <small><a href="http://en.wikipedia.org/wiki/List_of_countries_by_average_wage">(source)</a></small></p>

<p>How much is an unlocked iPhone 6 again? It starts around $649. That’s more than two months salary for the average Indian. The Galaxy S5? $799 or nearly three months of hard work. In places like India, feature phones are still quite prevalent. And even when a smartphone is introduced for their market, it pales in comparison to the sort of tech we are used to seeing.</p>

<figure id="fig-2014-11-06-06" class="media-container">
	<img src="https://www.aaron-gustafson.com/i/posts/2014-11-06/06-sm.jpg" srcset="/i/posts/2014-11-06/06-lg.jpg 1920w, /i/posts/2014-11-06/06-md.jpg 600w, /i/posts/2014-11-06/06-sm.jpg 320w" alt="The Samsung Galaxy S5 smartphone with a specification summary (relevant details follow)." />
</figure>

<p>Here we have <a href="http://www.samsung.com/gr/microsite/galaxys5/">Samsung’s flagship Galaxy S5</a> with an amazing set of specs. A 16GB camera?! A quad-core processor?! This is the stuff of dreams for anyone who has been working with computers for more than 10 years. My first desktop was a 5150.</p>

<p>No not <a href="https://en.wikipedia.org/wiki/5150_(album)">that awesome Van Halen record</a>, <a href="http://www-03.ibm.com/ibm/history/exhibits/pc25/pc25_intro.html">this beast from IBM</a>:</p>

<figure id="fig-2014-11-06-08" class="media-container">
	<img src="https://www.aaron-gustafson.com/i/posts/2014-11-06/08-sm.jpg" srcset="/i/posts/2014-11-06/08-lg.jpg 1920w, /i/posts/2014-11-06/08-md.jpg 600w, /i/posts/2014-11-06/08-sm.jpg 320w" alt="The IBM 5150 personal computer." />
</figure>

<p>It weighted over 20 lbs, 28 lbs with two floppy drives. The screen weighed another 13 lbs and the keyboard was 6 lbs. It maxed out at 256K of memory and offered 40K of read only memory. I couldn‘t even find a spec detailing how slow the processor was, but let’s just say that the computer I began my Web career with nearly 15 years later was only a Pentium 90 with something like 16 MB of RAM.</p>

<p>And here, this pocket-sized computer just blows all of that out of the water.</p>

<figure id="fig-2014-11-06-09" class="media-container">
	<img src="https://www.aaron-gustafson.com/i/posts/2014-11-06/09-sm.jpg" srcset="/i/posts/2014-11-06/09-lg.jpg 1920w, /i/posts/2014-11-06/09-md.jpg 600w, /i/posts/2014-11-06/09-sm.jpg 320w" alt="The Intex Cloud FX smartphone with specification summary (relevant details follow)." />
</figure>

<p>By contrast, here we have <a href="http://www.intexmobile.in/product_detail.aspx?PID=191&amp;PCatID=3">Intex’s Cloud FX</a>, a new phone with specs that read like the state of the art in 2007. A crappy camera, no front camera, a slow 1 GHz processor, a paltry 128 MB of RAM and barely double that in internal storage. It’s a crappy phone by our standards.</p>

<p>But that’s a Firefox OS phone aimed at the Indian market vs. an Android one aimed at the “developed” world. Perhaps you’d like to look at a more apples-to-apples comparison:</p>

<figure id="fig-2014-11-06-10" class="media-container">
	<img src="https://www.aaron-gustafson.com/i/posts/2014-11-06/10-sm.jpg" srcset="/i/posts/2014-11-06/10-lg.jpg 1920w, /i/posts/2014-11-06/10-md.jpg 600w, /i/posts/2014-11-06/10-sm.jpg 320w" alt="The BLU Dash Jr K smartphone with specification summary (relevant details follow)." />
</figure>

<p>Here we have the <a href="http://www.gsmarena.com/blu_dash_jr-5662.php">BLU Dash Jr K</a>.</p>

<p>Both it and the Galaxy S5 run Android 4.4 (Kitkat), but that’s where their similarities end. Look at the resolution of the Dash Jr K: 320×480 versus the 1920×1080 of the S5. Look at the processor speed. Look at the RAM.</p>

<p>Now, honestly, how many of you would willingly carry the Dash Jr K or the Cloud FX as your primary phone? Maybe as a laugh, maybe ironically, but I highly doubt many in our profession would subject themselves to that. Why? Because we don’t have to.</p>

<p>Now I don’t know your salary, but I’m willing to bet you make more money and have far more disposable income available to spend on cutting edge gadgets than most people in the world. Surely that’s the case when you compare us to China and India, but it’s equally true here in North America.</p>

<p>In the U.S., we see stats like “Smartphone sales accounted for nearly 85% of all mobile phone sales” and “Smartphones have reached 50% penetration” with relative frequency. But those headlines often lead us to draw incorrect conclusions about what devices people actually use to access the Web.</p>

<p>The dirty little secret behind that 50% penetration number is that the penetration in question was concentrated in a scant 30% of U.S. households. Kinda burying the lead if you ask me. <small><a href="http://www.chetansharma.com/usmarketupdateq12013.htm">(source)</a></small></p>

<figure id="fig-2014-11-06-12" class="media-container">
	<img src="https://www.aaron-gustafson.com/i/posts/2014-11-06/12-sm.jpg" srcset="/i/posts/2014-11-06/12-lg.jpg 1920w, /i/posts/2014-11-06/12-md.jpg 600w, /i/posts/2014-11-06/12-sm.jpg 320w" alt="A pie chart breakdown of the findings by the Pew Research Center on smartphone penetration by household income (relevant details follow)." />
</figure>

<p>The Pew Research Center released a study earlier this year that showed smartphone penetration in the US, broken down by income bracket. As expected, the higher the household income, the more likely you were to find someone with a smartphone.</p>

<p>In the &lt; $30,000 income bracket, smartphones were found in 47% of households. But it starts to get really interesting when you cross-reference that data with how many people fall into those income brackets. <small><a href="http://www.pewinternet.org/2014/02/27/part-1-how-the-internet-has-woven-itself-into-american-life/">(source)</a></small></p>

<figure id="fig-2014-11-06-13" class="media-container">
	<img src="https://www.aaron-gustafson.com/i/posts/2014-11-06/13-sm.jpg" srcset="/i/posts/2014-11-06/13-lg.jpg 1920w, /i/posts/2014-11-06/13-md.jpg 600w, /i/posts/2014-11-06/13-sm.jpg 320w" alt="The Pew Research Center’s findings when compared to U.S. Census Data (relevant details follow)." />
</figure>

<p>$30,000 was the average income in the U.S. in 2013. And, according to the 2010 census, the overwhelming majority of American households earn less than that. <small><a href="http://www.census.gov/2010census/data/">(source)</a></small></p>

<p>Now both the Samsung Galaxy S5 and the BLU Dash are technically smartphones, but one costs $43 and the other costs $799. On a limited budget, which do you think you’d be more likely to get?</p>

<p>Sure, in the US, carriers subsidize phone prices, but even the subsidized $199 AT&amp;T offers the S5 for (with a 2-year agreement) ends up costing $1319 once you factor in the $40 activation and the minimum of $45 a month for a data plan.</p>

<p>So again I’ll ask: On a limited budget, which do you think you’d be more likely to get?</p>

<p>So even if a household has a smartphone, there’s probably decent odds on it being something a little lackluster compared to what we are used to carrying.</p>

<p>While it may not be a big deal for us to pay $60, $100, or more a month for mobile data access with fast speeds and high bandwidth limits, that would be a burden for most people. It’s worth noting that the cheaper pay-as-you-go plans typically have substantially lower data caps, frequently cost more per megabit, and often run at far slower speeds. Accordingly, while the Galaxy S5 supports blazingly fast 4G LTE speeds, both the Cloud FX and the Dash Jr K run on 2G technology.</p>

<figure id="fig-2014-11-06-14" class="media-container">
	<img src="https://www.aaron-gustafson.com/i/posts/2014-11-06/14-sm.jpg" srcset="/i/posts/2014-11-06/14-lg.jpg 1920w, /i/posts/2014-11-06/14-md.jpg 600w, /i/posts/2014-11-06/14-sm.jpg 320w" alt="A photo of a Blackberry device experiencing an error loading a webpage because the page was too large. Photo Credit: Brad Frost." />
</figure>

<p>All of this is to say that we must be hyper-aware of how big our Web pages are. Large pages with tons of high-resolution images cost our users real money and, frankly, waste their time. Some might not even load. Big Web pages are a barrier to access.</p>

<p>Beyond page size, we should also be concerned with how much work we we are requiring of the browser. JavaScript-intensive sites and applications can run really poorly on devices with slow processors and minimal RAM, like the BLU Dash Jr K or the Intex Cloud FX.</p>

<p>These are just a few of the concerns we’re having to deal with today, and only about a third of our planet is online. There are 4.8 billion people with no Internet access. But it’s coming. And when it happens, we will likely have even more to deal with. Like language barriers.</p>

<figure id="fig-2014-11-06-15" class="media-container">
	<img src="https://www.aaron-gustafson.com/i/posts/2014-11-06/15-sm.jpg" srcset="/i/posts/2014-11-06/15-lg.jpg 1920w, /i/posts/2014-11-06/15-md.jpg 600w, /i/posts/2014-11-06/15-sm.jpg 320w" alt="A pie chart comparison of the global population that speaks English versus the percentage of the Web that is in English." />
</figure>

<p>Consider this: <a href="http://en.wikipedia.org/wiki/List_of_languages_by_total_number_of_speakers">About 11.7% of the world speaks English as its first or second language</a> yet <a href="http://en.wikipedia.org/wiki/Languages_used_on_the_Internet">55.7% of the Web is in English</a>. (French is spoken by roughly 1.4% of the world and 4% of the Web is in French.)</p>

<figure id="fig-2014-11-06-16" class="media-container">
	<img src="https://www.aaron-gustafson.com/i/posts/2014-11-06/16-sm.jpg" srcset="/i/posts/2014-11-06/16-lg.jpg 1920w, /i/posts/2014-11-16/15-md.jpg 600w, /i/posts/2014-11-06/16-sm.jpg 320w" alt="Pie charts detailing the percentage of people in India and China who speak English." />
</figure>

<p>This presents some significant challenges as the Web expands into places like India and China. Only 18.61% of India’s 1.2 billion people speak English as a first, second, or even third language. In China, only about 0.73% of their 1.2 billion people speak English. Network availability is only the first of many hurdles to accessing the Web for much of the planet. <small><a href="http://en.wikipedia.org/wiki/List_of_countries_by_English-speaking_population">(source)</a></small></p>

<p>We need to look beyond our technological and cultural bubble and consider how others experience the Web. As an industry, we must figure out how we can make their experiences better.</p>

<h2 id="ii-we-are-ux-professionals">II: We are UX professionals</h2>

<p>We are technologists who focus on accessibility, the capacity to tackle these challenges should come naturally to us. We were drawn to this field because we empathize with the struggles of others and want to help empower them to live independently.</p>

<p>We are user experience professionals and yet we’re often pigeon-holed outside of that practice. Our knowledge and contributions are often seen as only being applicable for people with “disabilities.” But our purview goes way beyond helping people with less than perfect vision, hearing, or mobility. Our purview is improving Web experiences for all people, regardless of physical or mental abilities, gender, race, or language.</p>

<p>Our purview is user experience and we need to assert ourselves and our role in that capacity.</p>

<p>More than most, we understand the importance of experience, of access, of independence because we work with people for whom “little things” like the ability to press a button can be a big problem. And beyond that, we also understand that experience is not a binary thing. <strong>It is a continuum.</strong></p>

<p>This is a crucial fact that the Web industry is only just beginning to come to terms with. <strong>We can help ease that transition.</strong></p>

<p>We are the champions of the egalitarian dream: equality of opportunity with the understanding that it does not guarantee equality of outcome or experience. We are pragmatic idealists who want to enable everyone access to amazing products and services.</p>

<p>We provide <em>tremendous</em> business value.</p>

<p>But we have a lot of work to do.</p>

<p>Sadly, many people still don’t value accessibility. They don’t get why it is important. They see it as expensive. They see it as a “nice to have”. They see it as an add-on.</p>

<p>I have gotten this reaction from designers. I have gotten it from developers. I have gotten it from other user experience professionals. And I have most often gotten it from managers and business owners. I’m sure you have as well.</p>

<p>I once had someone tell me he didn’t need to make his website accessible because he sold televisions and “blind people don’t watch TV.” I was floored. I mean holy crap!? This guy had no idea.</p>

<p>I had to educate him, but I needed to do it softly. I need to explain to him that his view of “special needs” was wrong. I had to be gentle because people don’t often react well to being told their world-view is fundamentally flawed. I’m sure I’m not the only one in this room who has been in a situation like this either.</p>

<p>If our primary job is to empower people to live independently, our second job is surely to educate the world, not just on how to make the Web more accessible, but why it matters. <strong>We need to bring everyone into the fold.</strong></p>

<figure id="fig-2014-11-06-17" class="media-container">
	<img src="https://www.aaron-gustafson.com/i/posts/2014-11-06/17-sm.jpg" srcset="/i/posts/2014-11-06/17-lg.jpg 1920w, /i/posts/2014-11-06/17-md.jpg 600w, /i/posts/2014-11-06/17-sm.jpg 320w" alt="A photo of the philosopher John Rawls." />
</figure>

<p>I love exercises that create opportunities for revelation. One of my favorites originates from John Rawls. Rawls was a philosopher who used to run a social experiment with students, church groups, and the like.</p>

<p>In the experiment, individuals were allowed to create their ideal society. It could follow any philosophy. It could be a monarchy or democracy or anarchy. It could be capitalist or socialist. The people in this experiment had free rein to control absolutely every facet of the society… but then he’d add the twist: They could not control what position they occupied in that society.</p>

<p>This twist is what John Harsanyi—an early game theorist—refers to as the <a href="https://en.wikipedia.org/wiki/Veil_of_ignorance">“Veil of Ignorance”</a> and what Rawls found, time and time again, was that individuals participating in the experiment would gravitate toward creating the most egalitarian societies.</p>

<p>It makes sense: what rational, self-interested human being would treat the elderly, the sick, people of a particular gender or race or creed or color, poorly if they could find themselves in that position?</p>

<p>We’re often put in a box and told to only concern ourselves with folks with “special needs.” Well news flash: <strong>we all have special needs</strong>. Some we’re born with. Some we develop. Some are temporary. Some have nothing to do with us personally, but are situational or purely dependent on the hardware we are using, the interaction methods we have available to us, or even the speed at which we can access the Internet or process data.</p>

<p>We need to look beyond the world of assistive Web technology and explain the value and insight we bring to approaches like Responsive Web Design. After all, what is RWD about if not access? Yes, its fundamental tenets are concerned with visual design, but in terms of the big picture, they’re all about providing the best possible reading experience. Responsive web design is also a perfect example of the continuum of experience we are so intimately familiar with.</p>

<p>We understand special needs. We understand fallbacks. And we understand how to design robust experiences that work under a wide variety of conditions. That knowledge is invaluable.</p>

<p><strong>We are invaluable.</strong></p>

<h2 id="iii-we-are-the-future">III: We are the future</h2>

<p>This is an incredibly exciting time to be working in accessibility. User experience is becoming central to how organizations work and how they design their products and accessibility should be at the core of that.</p>

<p><strong>This is our time!</strong></p>

<p>The more influence we have on the products and services our companies and clients create, the more places they can go and the more successful they will be.</p>

<p>Take WhatsApp for instance. Fundamentally, it is a chat application. That’s not terribly groundbreaking. But it developed into a way to avoid costly SMS messages. Still, even that’s not all that special: the App Store lists over 7,900 messaging apps for the iPhone.</p>

<p>What made WhatsApp matter was the shrewd business decision to move beyond the bubble. They chose to embrace access and embrace diversity. They made their messaging application available on a ton of platforms, especially low cost ones. So sure, they support iOS and Android, but unlike a lot of app developers, they officially support Android 2.1+, iOS 4.3+, Blackberry 4.7+, Symbian, Nokia Series 40, Windows Phone. Some of those aren’t even smartphone OSes!</p>

<p>While many may not consider this an “accessibility” win, it absolutely is. WhatsApp made a decision to open up access to their messaging application to people who were traditionally ignored by mobile app developers. And they were rewarded handsomely for this: as of last count, they had somewhere around 600 million users globally. And then there’s that little thing about them selling to Facebook for $19 billion.</p>

<p>And WhatsApp isn’t a fluke in benefiting from making itself more accessible: China’s WeChat boasts a user base of 600 million and Japan’s LINE has over 400 million users. All of these messaging platforms have benefitted greatly from embracing devices and technologies available to people outside of our bubble.</p>

<p>We can and should be advising our companies and clients on why and how to be more accessible. We need to look at the big picture and we should not be afraid to be bold in asserting that <strong>accessibility creates opportunity</strong>.</p>

<p>We already know that strong content guidelines pay dividends by creating opportunities for our content to work harder for us. Not only do they improve the readability of content on the sites we build, but they facilitate social sharing through more engaging summaries and headlines.</p>

<p>The clear, well-written, jargon-free content we advocate for is easier to translate into other languages. It also makes the content easier to follow via screen readers and other vocalization tools like <a href="http://www.readspeaker.com/">Readspeaker</a>, which in turn makes it possible to offer novel ways of accessing our content, like automated podcasts.</p>

<p>Our focus on semantic, meaningful, markup allows our content to be pulled into other contexts including focused reading apps like Pocket, Readability, and Instapaper.</p>

<p>And while we can certainly do a lot to make rich, JavaScript-based interactions far more accessible to assistive technology, our advocacy for progressive enhancement ensures that our content and tools work no matter what.</p>

<figure id="fig-2014-11-06-18" class="media-container">
	<img src="https://www.aaron-gustafson.com/i/posts/2014-11-06/18-sm.jpg" srcset="/i/posts/2014-11-06/18-lg.jpg 1920w, /i/posts/2014-11-06/18-md.jpg 600w, /i/posts/2014-11-06/18-sm.jpg 320w" alt="Headline: “Sky’s internet service mistakenly blocks web-critical plugin” (Source: The Guardian)" />
</figure>

<p>Let’s say an ISP blocks jQuery as malware. No problem.</p>

<figure id="fig-2014-11-06-19" class="media-container">
	<img src="https://www.aaron-gustafson.com/i/posts/2014-11-06/19-sm.jpg" srcset="/i/posts/2014-11-06/19-lg.jpg 1920w, /i/posts/2014-11-06/19-md.jpg 600w, /i/posts/2014-11-06/19-sm.jpg 320w" alt="Headline: “FCC to Marriott: No, you can’t force your customers onto terrible hotel WiFi” (Source: The Washington Post)" />
</figure>

<p>Let’s say the page is taking a long time to download on a high-latency mobile network (or hotel Wi-Fi). No big deal.</p>

<p>The products we build just work because we know that we don’t control how they are delivered.</p>

<p>It’s our job to educate others on this reality and to demonstrate why these are central to user experience.</p>

<h2 id="iv-we-are-agents-of-change">IV: We are Agents of Change</h2>

<p>The shift to handheld computers has been huge for accessibility. After all, the computers in our pockets are assistive technology. This is our world!</p>

<p>I’m going to make a somewhat bold prediction: while touch has been revolutionary in many ways toward improving digital access, voice is the future. And the user experience of voice-based interfaces is going to be critical in creating more opportunities for people to interact with and participate in the digital world.</p>

<p>We’ve got the jump on the other folks working in user experience when it comes to voice: We’ve been considering how interfaces sound for years. On top of that, we already understand how to design alternate interaction methods because we see experience as a continuum.</p>

<p>As voice UX technology—for example, Siri, Google Now, and Cortana—improves, we should be the ones people should look to as the experts. We will empower the next generation of websites and applications to become voice-enabled. And in so doing, we will improve the lives of billions. Because “accessibility” is not about disabilities, it’s about access and <strong>it’s about people</strong>.</p>

<p>Sure, we’ll make it easier to look up movie times and purchase tickets to see the latest <cite>Transformers</cite> debacle, but we will also empower the nearly 900 million people globally—over 60% of whom are female—that are illiterate. And that’s a population we have not traditionally viewed as our purview either.</p>

<p>We will create new opportunities for the poor and disadvantaged to participate in a world that has largely excluded them. You may not be aware, but 80% of Fortune 500 companies—think Target, Walmart—only accept job applications online or via computers.</p>

<p>We will enable people who have limited computer skills or who struggle with reading to apply for jobs with these companies.</p>

<p>We will empower immigrants to read lease agreements and their postal mail.</p>

<p>We will enable people with visual disabilities to vote, even on paper ballots, without human assistance.</p>

<p>We can help bridge the digital divide and the literacy gap. We can create opportunities for people to better their lives and the lives of their families. We have the power to create more equity in this world than most of us have ever dreamed.</p>

<p>This is an incredibly exciting time, not just for the accessibility experts, not just for user experience, not just the Web, but for the world! I can’t wait to see how awesome you make it!</p>

<figure id="fig-2014-11-06-20" class="media-container">
	<img src="https://www.aaron-gustafson.com/i/posts/2014-11-06/20-sm.jpg" srcset="/i/posts/2014-11-06/20-lg.jpg 1920w, /i/posts/2014-11-06/20-md.jpg 600w, /i/posts/2014-11-06/20-sm.jpg 320w" alt="A photo of the opening ceremony of the 2012 London Olympic Games when Sir Tim Berners-Lee (creator of the World Wide Web) typed “This is for Everyone” across the stadium." />
</figure>

<p>Thank you.</p>
]]></content>
  </entry>
  
</feed>
