<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Browse by Tag: Accessibility | Aaron Gustafson]]></title>
  <link href="https://www.aaron-gustafson.com/notebook/tags/accessibility/atom.xml" rel="self"/>
  <link href="https://www.aaron-gustafson.com/"/>
  <updated>2016-03-05T06:39:35+00:00</updated>
  <id>https://www.aaron-gustafson.com/</id>
  <author>
    <name><![CDATA[Aaron Gustafson]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Learn From the Past, Enhance for the Future]]></title>
    <link href="https://www.aaron-gustafson.com/notebook/learn-from-the-past-enhance-for-the-future/"/>
    <updated>2016-03-04T22:33:57+00:00</updated>
    <id>https://www.aaron-gustafson.com/notebook/learn-from-the-past-enhance-for-the-future</id>
    <content type="html"><![CDATA[<p><em>I had the great pleasure of delivering the closing keynote for the first EnhanceConf. I wanted to talk about voice and the future of “headless” user interfaces. Here’s what I had to say.</em></p>

<!-- more -->

<hr />

<p>Early last year, <a href="/notebook/how-to-apply-progressive-enhancement-when-javascript-seems-like-a-requirement/">a cry for help on Stack Overflow drew my attention</a>:</p>

<blockquote>
  <p>I’ve been trying to make my site … work fully without JavaScript, however, I’ve found myself in situations where I can’t honestly think how I would do some features without it.</p>
</blockquote>

<p>The submitter, JamHam, is certainly not alone in feeling this way. The ways we build websites change all the time. When I started out, it was pretty simple: you had HTML. Lots and lots of HTML. We also had Java applets, then Shockwave and Flash. Then we got some very basic stylesheet support. Then JavaScript.</p>

<p>As the years pressed on, the three major technologies underpinning the Web—HTML, CSS, and JavaScript—evolved and became even more powerful.</p>

<p>Things coalesced for a while in the early oughts before Jesse James Garrett re-christened a relatively obscure Microsoft creation, <code>XMLHttpRequest</code>, “AJAX” and set countless designers hearts aflutter with the promise of banishing the page refresh. At the heart of this revolution was JavaScript, and companies began betting their entire Web presence on its availability. Most learned that wasn’t such a good idea and began using it as an enhancement to the experience rather than a requirement.</p>

<p>After Ajax, there was HTML5, CSS3, and a host of new JavaScript APIs… the JavaScript frameworks—Angular, Knockout, Backbone, Ember, React… The ways we can create Web products just keep changing; sometimes slowly, but more often than not at such a speedy clip it leaves my head spinning.</p>

<p>The one thing I’ve learned however, being an “old man” in Web terms, is that web design is cyclical, just like everything else. <strong>The challenges we face building web products today are not new challenges.</strong> Moreover, the lessons we learned building similar products in the “Web 1.0” days pay dividends today and will continue to do so in the future.</p>

<p>When I started out on the Web, I had a 28.8 <abbr aria-label="kilobits per second">kbit/s</abbr> modem, but still had to support users on 14.4 <abbr aria-label="kilobits per second">kbit/s</abbr> connections. That’s half the speed I was used to running at. That may have been 20 years ago, but the lessons I learned about streamlining my HTML, optimizing images, and minimizing downloads has helped me immeasurably when dealing with high-latency mobile networks and excruciatingly slow “broadband” connections.</p>

<p>(I’m looking at you, every hotel ever.)</p>

<p>When I started out on the Web, I had an 800x600 monitor, but still had to support 640x480 screen resolutions. I learned the importance of prioritizing content long before media queries and flexbox enabled us to adapt our layouts on the fly. And while our computer screens keep getting bigger, mobile devices and wearables present the very same challenges I was tackling with 640x480, but in even tighter confines.</p>

<figure id="figure-2016-03-04-01">
{% adaptive_image /i/posts/2016-03-04/01.gif %}
</figure>

<p>When I started out on the Web, there was no JavaScript. All calculations, data processing, and dynamic functionality had to be handled by the server. I learned how to process web forms in Perl, later trading in my CGI scripts for PHP, Ruby, and Python. And while the vast majority of our users today have JavaScript baked into their browsers, I still rely on server-side fallbacks because I recognize that we don’t control the execution environment on the open Web.</p>

<blockquote>
  <p>The Web is the most hostile software engineering environment imaginable.<br />— Douglas Crockford</p>
</blockquote>

<p>You’re a savvy bunch, so I’m sure none of this is news to you, but I wanted to set the stage for what I’m really here to talk about. There’s a new cycle about to hit us and chances are you might not be thinking about it yet: Voice.</p>

<h2 id="i-the-headless-ui">I: The Headless UI</h2>

<p>Science fiction has often been a strong predictor of our technological future. HAL 9000 from <em>2001: A Space Odyssey</em> is probably the most (in)famous example of a computer that interacts with its users largely via voice. As a concept, the “talking computer” has appeared time and time again in space-age fiction—everything from <em>Red Dwarf</em> to <em>Interstellar</em>.</p>

<p>To function in the real world like they do on TV and in the movies, computers need two capabilities: Natural language processing (to understand what we say) and speech synthesis (to communicate, aurally, back to us).</p>

<figure id="figure-2016-03-04-02">
{% adaptive_image /i/posts/2016-03-04/02.gif %}
</figure>

<p>Natural language processing has its roots in the 1950s, but many of these early speech models were limited because they were built around a series of hard-coded rules that the computers followed. In the 1980s, however, machine learning and real-time statistical analysis became possible.</p>

<p>As hardware capabilities continued to improve and computers became more powerful, they got better at recognizing the words we were saying to them. Eventually, and with enough processing power, they also began to assign meaning to words and could react accordingly.</p>

<p>As the years marched on, the overhead required to enable our projects to listen to our users has dropped significantly.</p>

<p>Listening is great, but true communication is bidirectional. Humans have been experimenting with speech synthesis since the late 1700s, but it wasn’t until the 1980s that we got a decent result though. By the 1990s, reasonably intelligible text-to-speech software was being rolled out alongside most operating systems as a core component of their assistive technology offerings: The “screen reader”. At present, screen readers are probably the best indicator of what the future of voice interaction will sound like.</p>

<p>When combined, the ability of a computer to listen and respond gave rise to virtual personal assistants like Siri, Cortana, Alexa, and more.</p>

<p>Over time, our customers will become more accustomed to and reliant on voice-based interactions with their computers and the Web. Enabling them to complete critical tasks without a visual user interface will be crucial for the long-term success of our Web-based products.</p>

<p>So how do you design a “headless” UI? That’s easy: You design the conversation.</p>

<h2 id="ii-interface-is-conversation">II: Interface is Conversation</h2>

<p>Let’s take a trip back in time to one of the earliest computer games: Zork. Zork was written between 1977 and 1979. It’s a text-based adventure game that operates a lot like a game of <em>Dungeons &amp; Dragons</em>—with the program serving the role of gamemaster.</p>

<blockquote>
  <p>West of House<br /> You are standing in an open field west of a white house, with a boarded front door.<br /> There is a small mailbox here. 
$&gt; <strong>open mailbox</strong></p>
</blockquote>

<p>As you move from location to location throughout the game, the program describes the environment and notes objects and people you can interact with. You type what you want to do and the program tells you the results of your actions.</p>

<p>As this was the early days of computer gaming, you might think Zork’s interactions would be simple noun-verb combinations—”kill troll”—but Zork was more sophisticated than that. Its parser was could understand far more complex commands like “hit the troll with the Elvish sword”. This made the experience far more natural, as if you were playing a table top game with friends.</p>

<p>Whether Zork or a webpage, <strong>every interface is a conversation</strong>—we engage our users directly in an effort to inform them, entertain them, or persuade them to act in a particular way. How this conversation goes directly affects the experience our users have.</p>

<p>Let’s look at a few web page and interface component types to identify the kinds of conversations we trying to have with our users in each:</p>

<ul>
  <li><strong>Homepage</strong><br /> We’ve just met and I’m explaining what you can do on my site (and, in some cases, why it matters).</li>
  <li><strong>Contact Form</strong><br /> You’re asking or telling me something. I want to help you. It’s common courtesy for me to let you know how long it may take me to get back to you with a response; and for me to abide by that.</li>
  <li><strong>Product Page</strong><br /> I’m explaining what a particular object or service is, what it does, and how it will benefit you. I should “show” you why something is great rather than “tell”-ing you that it is because you’re immune to salesy <abbr aria-label="bullshit">BS</abbr>.</li>
  <li><strong>Status Update</strong><br /> I may prompt you with a question, but I’m here to listen. The floor is yours. (But I’m probably mining what you say for data so I can market to you later.)</li>
</ul>

<p>When we approach interfaces as conversations, we humanize our products and improve our users’ experiences. When we don’t, things can fall apart quickly…</p>

<p>Over the 2011 holidays, Facebook users were uploading photos like crazy. In the span of a few days, Facebook processed more photo uploads than are contained in the entirety of Flickr. Seriously, that’s a lot of photos.</p>

<p>One unintended consequence of this deluge of photo uploads was a significant uptick in people asking Facebook to remove specific ones. Facebook received millions of these “photo reports”, but they made no sense: Moms holding babies reported for harassment, pictures of puppies reported for hate speech, and so on. Roughly 97% of these photo reports were dramatically mis-categorized.</p>

<p>Facebook’s engineers reached out to some of the users who had reported these photos to get a bit more background regarding their submissions. At the time Facebook’s photo reporting interface provided a list of reasons users could choose from if they wanted a photo removed, but, as Facebook soon discovered, many of the reports were made because users didn’t want the photo posted for reasons other than those provided. In some cases, it was because they didn’t like how they looked in the photo. In others, it was because the photo was of an ex-partner or even a beloved pet they’d shared with an ex-boyfriend or ex-girlfriend.</p>

<p>The existing photo reporting tool had not done a good job of accounting for these more personal reasons for wanting a photo removed, so the Facebook engineers went to work. They added a step that asked <em>How does this photo make you feel?</em> The options were simple:</p>

<ul>
  <li>Embarrassing</li>
  <li>Upsetting</li>
  <li>Saddening</li>
  <li>Bad Photo</li>
  <li>Other</li>
</ul>

<p>The “other” option also provided a free-response text field to fill in.</p>

<p>With this system in place, they found that 50% of reporters who answered the new question chose one of the provided options. That was pretty helpful, but there was still a problem: 34% of the “other” respondents were writing “It’s embarrassing” in the blank rather than choosing the “embarrassing” option already provided.</p>

<p>What the Facebook team realized was that people were not identifying with the “embarrassing” text (or may have even thought it was referring to them, rather than assuming an implied “It’s”). A subtle shift in language was needed, so they changed the label to <em>Please describe the photo</em> and they updated the options to mirror how people actually talk:</p>

<ul>
  <li>It’s embarrassing</li>
  <li>It’s a bad photo of me</li>
  <li>It makes me sad</li>
</ul>

<p>With this subtle change, they were able to increase the percentage of photo reporters who chose one of the options provided to a whopping 78%.</p>

<p>Words matter. Even in something as simple and banal as a form, the words we choose set the tone for our users’ experiences and often have an affect on what they do… or fail to do.</p>

<p>The text of our interfaces—especially form labels and responses—is just one small part of the content picture, but it’s a perfect example of how easy it can be to overlook conversation in our interfaces. There are many other types of content like product descriptions, marketing copy, legal statements, visualizations, video, audio, and more. Content is where experience begins. It’s the core that we seek to progressively enhance. It’s also the foundation upon which the voice-based experiences of the future will be based.</p>

<p>The more time and consideration we put into how our interfaces read, the better-positioned we will be to succeed in the future of headless UIs. Once stripped of its beautifully-crafted, responsive layout, engaging animations, and artful illustrations, does your site hold up?</p>

<hr />

<p>Back in 2006, <a href="http://www.dustindiaz.com/naked-day/">Dustin Diaz proposed CSS Naked Day</a>—a day when sites could be stripped of their visual design to showcase their content, semantics, and organization.</p>

<blockquote>
  <p>It will be a test case to see how usable your website is to others without a “design”.<br /> —Dustin Diaz</p>
</blockquote>

<p>“Design”, as Dustin was refering to it, is the visual design of a site, but design is not solely concerned with visual representations. Diving into etymology for a moment here, <em>design</em> comes from the Latin <i lang="la">designare</i> meaning “to mark out or indicate”. The purpose of design is not to make something pretty, it’s to clarify.</p>

<p>If the words we use form the basis of the conversations we have with our users, the semantics we employ clarify that meaning. Choosing elements with semantic value enriches our content, illuminating the meaning and intent of our words in order to overcome the limitations of text and bring it up to par with spoken language. After all, they may look the same visually, but there’s a big difference between these two statements:</p>

<p>{% gist 6f5b7c0f0c072631a908 different-meanings.html %}</p>

<p>Beyond using markup to clarify the intent of the words we write, we can use it to spell out relationships that are often represented visually. Dustin described one way we do this as part of the impetus for CSS Naked Day (emphasis mine):</p>

<blockquote>
  <p>In the spirit of promoting Web Standards along with good semantic markup and <em>proper hierarchy structures</em></p>
</blockquote>

<p>By “proper hierarchy”, Dustin is talking about the document outline. A document outline is created through use of heading elements (<code>h1</code>–<code>h6</code>). It provides a easy way to review the organization of our web pages and validate our source order decisions. It also helps us ensure the flow works, which is incredibly important in any conversation. It helps us get to the point, streamline our content, and remove distractions… all of which are a sign of respect to our users.</p>

<p>None of this is news, of course, content strategists have been recommending that we streamline our content since the dawn of the Web. Sadly, many folks didn’t heed that advice until they were forced to confront the often infuriating world of mobile. Smaller screens required focused content.</p>

<p>When Luke Wroblewski coined “mobile first”, he told us to focus on the core purpose each and every page. He was, in essence, telling us to focus on the conversation we are having with our users. This approach pays huge dividends on small screens, but when it comes to voice-based interactions, “the page” doesn’t really exist. Experience is the sum of each individual interaction. As part of their <a href="https://developer.amazon.com/public/solutions/alexa/alexa-skills-kit">Alexa Skills Kit</a>, <a href="https://developer.amazon.com/public/solutions/alexa/alexa-skills-kit/docs/alexa-skills-kit-voice-design-best-practices">Amazon offers a ton of recommendations for designing for voice</a>, many of which happen to be equally useful for sighted users.</p>

<h3 id="write-for-people">Write for People</h3>

<p>We don’t author content for ourselves. We write for others. If what we write frustrated or alienates our users, we’ve failed at our job. In their profoundly helpful book <a href="http://www.amazon.com/gp/product/0321988191/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=0321988191&amp;linkCode=as2&amp;tag=easydesign-20&amp;linkId=5INOUNG72ODCWZQV"><cite>Nicely Said</cite></a>, Nicole Fenton and Kate Kiefer Lee offer numerous suggestions for how to write with the reader in mind:</p>

<blockquote>
  <ul>
    <li>Be clear.</li>
    <li>Be concise.</li>
    <li>Be honest.</li>
    <li>Be considerate.</li>
    <li>Write how you speak.</li>
  </ul>
</blockquote>

<p>They also make the recommendation that you read your work aloud. As we head into the world of voice-based interactions, that’s beta testing!</p>

<h3 id="avoid-technical-and-legal-jargon">Avoid Technical and Legal Jargon</h3>

<p>When we are writing for our readers, we need to be familiar with their level of domain knowledge so we don’t frustrate or alienate them. For example, if you track error codes for issues on your site, send them to <em>your developers</em>, but never present them to a user.</p>

<figure id="figure-2016-03-04-03">
{% adaptive_image /i/posts/2016-03-04/03.png %}
</figure>

<p>Similarly, we should avoid legalese and write in plain language. Medium has done a great job of this with <a href="https://medium.com/policy/medium-terms-of-service-9db0094a1e0f#.mgexdk816">their Terms of Service</a>.</p>

<figure id="figure-2016-03-04-04">
{% adaptive_image /i/posts/2016-03-04/04.png %}
</figure>

<h3 id="when-requesting-feedback-make-it-clear-that-the-user-needs-to-respond"><strong>When Requesting Feedback, Make It Clear that the User Needs to Respond</strong></h3>

<p>In perhaps the most common form example, consider the label “First Name”. It’s not terribly conversational and doesn’t beg for a response. Labels like “What is your first name?” make it clear the user should respond.</p>

<p>{% gist  better-labels.html %}</p>

<figure id="figure-2016-03-04-05">
<audio controls="">
<source src="https://www.aaron-gustafson.com/i/posts/2016-03-04/05.mp3" type="audio/mp3" />
<source src="https://www.aaron-gustafson.com/i/posts/2016-03-04/05.ogg" type="audio/oga" />
</audio>
</figure>

<p>Similarly, when there’s an error, notify them of the error and, if possible, give them some clues on how to fix it.</p>

<p>{% gist 6f5b7c0f0c072631a908 field-error.html %}</p>

<figure id="figure-2016-03-04-06">
<audio controls="">
<source src="https://www.aaron-gustafson.com/i/posts/2016-03-04/06.mp3" type="audio/mp3" />
<source src="https://www.aaron-gustafson.com/i/posts/2016-03-04/06.ogg" type="audio/oga" />
</audio>
</figure>

<h3 id="when-asking-a-user-to-choose-clearly-present-the-options"><strong>When Asking a User to Choose, Clearly Present the Options</strong></h3>

<p>This comes into play often when dealing with forms. Ensuring radio and checkbox controls are properly associated with their labels is critical.</p>

<p>{% gist 6f5b7c0f0c072631a908 radio-label.html %}</p>

<p>You can also use the <code>fieldset</code> and <code>legend</code> elements to group the related controls, but be sure to make the <code>legend</code> focusable or associate it with the first focusable form control in order to ensure the question is read out.</p>

<p>{% gist 6f5b7c0f0c072631a908 fieldset.html %}</p>

<figure id="figure-2016-03-04-07">
<audio controls="">
<source src="https://www.aaron-gustafson.com/i/posts/2016-03-04/07.mp3" type="audio/mp3" />
<source src="https://www.aaron-gustafson.com/i/posts/2016-03-04/07.ogg" type="audio/oga" />
</audio>
</figure>

<p>We should strive for the same sort of clarity when presenting navigation options. The HTML5 <code>nav</code> element enables us to semantically identify an area of the page being used for navigation. It does not, however, identify the <code>nav</code> element as being for navigation when encountered naturally in the flow of the document. For that reason, it can be useful to provide an textual introduction to the section, even if you choose to visibly hide it. You might even consider expanding the text of your navigation items to provide additional context.</p>

<p>{% gist 6f5b7c0f0c072631a908 navigation.html %}</p>

<figure id="figure-2016-03-04-08">
<audio controls="">
<source src="https://www.aaron-gustafson.com/i/posts/2016-03-04/08.mp3" type="audio/mp3" />
<source src="https://www.aaron-gustafson.com/i/posts/2016-03-04/08.ogg" type="audio/oga" />
</audio>
</figure>

<h3 id="prompts-should-be-short-while-still-being-clear">Prompts Should be Short, While Still Being Clear.</h3>

<p>In <a href="https://www.stmarys-ca.edu/sites/default/files/attachments/files/On_The_Method_of_Theoretical_Physics.pdf">a 1933 lecture at Oxford</a>, Albert Einstein famously said</p>

<blockquote>
  <p>It can scarcely be denied that the supreme goal of all theory is to make the irreducible basic elements as simple and as few as possible without having to surrender the adequate representation of a single datum of experience.</p>
</blockquote>

<p>Or, as <a href="https://books.google.com/books?id=prDfAFjet9cC&amp;lpg=PR7&amp;ots=PA9rRog4cr&amp;dq=How%20a%20%E2%80%98Difficult%E2%80%99%20Composer%20Gets%20That%20Way&amp;pg=PA230#v=onepage&amp;q&amp;f=false">Roger Sessions paraphrased it</a></p>

<blockquote>
  <p>Everything should be as simple as it can be but not simpler.</p>
</blockquote>

<p>Clear and concise writing is the hallmark of great content. We need to resist the urge to write for writing’s sake. We write in the service our audience, not for ourselves.</p>

<p>Government websites are some of the worst offenders in this area. Consider this lovely passage:</p>

<blockquote>
  <p>Heavy rains throughout most of the State have given an optimistic outlook for lessened fire danger for the rest of the season. However, an abundance of lightning maintains a certain amount of hazard in isolated areas that have not received an excessive amount of rain.</p>
</blockquote>

<p>It could be written far more clearly as</p>

<blockquote>
  <p>Heavy rains throughout most of the State have lessened fire danger for the rest of the season. However, lightning threatens isolated dry areas.</p>
</blockquote>

<p>Here in the UK, the Government Digital Service has made great strides overhauling excruciatingly painful content and making it easier to read and understand. One such example is <a href="https://gds.blog.gov.uk/2014/07/28/doing-the-hard-work-to-make-things-simple/">their overhaul of the Accelerated Possession process</a> that allows landlords to evict a tenant.</p>

<p>The original paper form asked for the address like this</p>

<blockquote>
  <p>The claimant seeks an order that the defendant(s) give possession of:<br /> (If the premises of which you seek possession are part of a building identify the part eg. Flat 3, Rooms 6 and 7)</p>
</blockquote>

<p>Before requesting the type of property concerned</p>

<blockquote>
  <p>(‘the premises’) which is<br /> ☐ a dwelling house<br /> ☐ part of a dwellinghouse</p>
</blockquote>

<figure id="figure-2016-03-04-09">
{% adaptive_image /i/posts/2016-03-04/09.png %}
</figure>

<p>Clear and to the point, right?</p>

<p>The GDS went to work and streamlined the process in plain language:</p>

<blockquote>
  <p>What kind of property do you want to take back?<br /> ◎ A self-contained house, flat or bedsit<br /> ◎ Room or rooms in a property.<br /> Tenants may share kitchen or bathroom</p>
</blockquote>

<p>Then they allow you to lookup the property or manually enter the address.</p>

<figure id="figure-2016-03-04-10">
{% adaptive_image /i/posts/2016-03-04/10.png %}
</figure>

<p>While not specifically designed for the future of headless UIs, this form is prepared for their eventuality.</p>

<h3 id="ask-only-necessary-questions">Ask Only Necessary Questions</h3>

<p>We show our users respect by respecting their time. Obviously straightforward, brief writing is one way we do that, but another is to reduce the time it takes to complete a task. Many forms are brimming with fields to be filled in. In some cases, the vast majority are purely optional. And while it may be easy to spot the required fields visually, bypassing them in an aural interface can be incredibly difficult.</p>

<figure id="figure-2016-03-04-11">
{% adaptive_image /i/posts/2016-03-04/11.png %}
</figure>

<p>User experience designers have been pushing for simplified forms since… well, as long as I can remember. Users appreciate them, they tend to result in better data, and they also tend to convert better than long forms. And when it comes to voice-based interactions, they will become a necessity. No one is going to want to spend 15 minutes working their way through a 15 question registration form when all that’s required is their email address and for them to choose a password.</p>

<figure id="figure-2016-03-04-12">
{% adaptive_image /i/posts/2016-03-04/12.png %}
</figure>

<p>On a similar note, we should avoid slicing fields into multiple parts if at all possible. For instance, you still see fields like this one, asking for a US phone number, quite often:</p>

<figure id="figure-2016-03-04-13">
{% adaptive_image /i/posts/2016-03-04/13.png %}
</figure>

<p>When interacting with this construct via voice, a user will be required to supply three separate values. In order to do so, each field would require a label. Even in the States, most developers would only know how to label the first of those three boxes. (They are area code, exchange or central office code, and line number, if you’re interested.)</p>

<p>HTML5 introduced a host of new field types that consolidate phone numbers, dates, times, and other complex data types into single fields. Use them! As an added bonus, most enforce content validation and formatting rules for you automatically.</p>

<h3 id="present-information-in-consumable-pieces">Present Information in Consumable Pieces</h3>

<p>Like computers, we humans have a finite amount of “working memory”. The amount of mental resources required to operate an interface is called its “cognitive load”. When the amount of information we need to process exceeds our capacity to handle it, we can miss important details, have trouble concentrating, and become frustrated.</p>

<p>We deal with cognitive load in GUI design all the time, but in voice-based interactions, there are no visuals to act as signposts and provide reminders about where we are and what we’re doing. This is why it is critical to break complicated tasks down into simpler ones and eliminate excess noise (like non-required fields). We can also reduce cognitive load by chunking search results and other list-type content into small groups, asking the user if they want more before loading and presenting them.</p>

<blockquote>
  <p>The top seller in the garden department is Repel Lemon Eucalyptus Natural Insect Repellent, 4-Ounce Pump Spray</p>
</blockquote>

<blockquote>
  <p>Would you like to hear the rest?</p>
</blockquote>

<h2 id="iii-future-enhancements">III: Future Enhancements</h2>

<p>Paying attention to how our interfaces read is critical to success in the future of voice-based interactions. Thankfully, we already view content as the centerpiece of every progressively enhanced experience. But we can go further.</p>

<p>Both Microsoft and Amazon have given us the tools to voice-enable our websites beyond the HTML we present. Amazon has chosen to do this via a dedicated JSON API, through which we can “teach” Alexa “skills”. Using this API, you can enable your users to access core site functionality through the Echo, FireTV, or any other device that has integrated the Alexa Voice Service.</p>

<p>Microsoft has taken a slightly different approach. Using a relatively simple XML format, they have enabled us to teach Cortana new commands that tie directly into our website.</p>

<p>{% gist 6f5b7c0f0c072631a908 meta.html %}</p>

<p>All we need to do is include a <code>meta</code> tag pointing to an XML file that details the commands (and variations) and, when a user installs the site as a hosted app, Cortana picks up the new commands automatically. Those commands, when issued, can open a specific page or even kick off JavaScript methods in the target page.</p>

<p>{% gist 6f5b7c0f0c072631a908 vcd.xml %}</p>

<hr />

<p>We are just starting to scratch the surface of what’s possible in voice-enabling the Web, but it’s exciting to see how some companies are addressing this opportunity. It’s always interesting when things come full circle and we see how lessons we learned early on in the Web remain applicable, not matter how much or quickly things seem to change. Seeing this pattern repeat time and time again is why I’m so drawn to the philosophy of progressive enhancement; it’s not only concerned with supporting the past… it’s setting us up for success in the future.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Affirming User Choice With Checkboxes]]></title>
    <link href="https://www.aaron-gustafson.com/notebook/affirming-user-choice-with-checkboxes/"/>
    <updated>2016-01-06T19:24:31+00:00</updated>
    <id>https://www.aaron-gustafson.com/notebook/affirming-user-choice-with-checkboxes</id>
    <content type="html"><![CDATA[<p>“Checkbox” form controls have long been a part of software. They enable users to provide a simple binary response—yes or no. On the Web, we often see them in two scenarios: confirmations and multiple choice.</p>

<!-- more -->

<h2 id="confirmation-checkboxes">Confirmation Checkboxes</h2>

<p>Standalone checkboxes are often employed to enable users to affirm a statement, as in <a href="#fig-2016-01-06-01">this example</a> from <a href="https://online.americanexpress.com/myca/logon/us/action/LogonHandler?request_type=LogonHandler&amp;Face=en_US#lilo_loginForm">the American Express login form</a> where a customer can indicate they’d like the site to remember them.</p>

<figure id="fig-2016-01-06-01" class="media-container">{% adaptive_image /i/posts/2016-01-06/01.png %}<figcaption>American Express’ login form offers users the option of being remembered. As that is a binary choice (e.g. yes or no), the checkbox makes sense.</figcaption></figure>

<p>Here’s a simplification of the markup they’re using:</p>

<p>{% gist d281f889a11634b45280 american-express-login-simplified.html %}</p>

<p>This works really well, though I generally prefer to <a href="https://www.aaron-gustafson.com/notebook/labeled-with-love/#an-alternate-approach">combine explicit and implicit labeling</a> to simplify my CSS selectors and broaden their applicability. Here’s how I would rewrite this control:</p>

<p>{% gist d281f889a11634b45280 american-express-login-reimagined.html %}</p>

<p>Regardless of the markup pattern itself, it’s important to note the explicit association of the form control and the <code>label</code> element (using the <code>for</code> attribute). You’ll also notice the input has a straightforward <code>name</code> value which will be submitted to the back end if the user ticks the box.</p>

<p>It’s worth noting that some back-end systems may require a value be submitted for the given variable name (in this case, “REMEMBERME”) regardless of whether the user has ticked the checkbox. If that’s a requirement, you can alter the pattern to use a hidden <code>input</code> as well:</p>

<p>{% gist d281f889a11634b45280 american-express-login-with-hidden.html %}</p>

<p>The source order matters because with matching <code>name</code> values, the final submittable <code>value</code> will always be the one the back-end receives. With this setup, the <code>value</code> of “no” (from the hidden <code>input</code>) will be submitted by default. If the checkbox is ticked, its <code>value</code> is submitted instead, setting REMEMBERME to “yes”.</p>

<h2 id="multiple-choice-checkboxes">Multiple Choice Checkboxes</h2>

<p>The other way we often see checkboxes used is to enable users to choose zero or more items from a collection of options. Consider <a href="#fig-2016-01-06-02">this example</a> from <a href="http://chadevicelab.org/book-time">the Chattanooga Open Device Lab’s reservation form</a>. It allows users to choose the devices they’d like to include in their test matrix:</p>

<figure id="fig-2016-01-06-02" class="media-container">{% adaptive_image /i/posts/2016-01-06/02.png %}<figcaption>In this excerpt from the reservation form on the Chattanooga Open Device Lab website, users can choose to include gaming system options.</figcaption></figure>

<p>The markup they employ is pretty well-organized and straightforward: it’s a list of checkbox options.</p>

<p>{% gist d281f889a11634b45280 chaodl-checkbox-list.html %}</p>

<p>As this is an instance where a user could choose more than one option, the back end needs to be able to capture that information in what’s called an “array”. An array, if you’re unfamiliar, is a collection of values. You’ll notice that the <code>name</code> given to each of these checkbox <code>input</code> elements is the same: “reservation_requested_device[]”. The square brackets (“[]”) at the end of the <code>name</code> are the magic bit that allows the values of <em>each</em> chosen “reservation_requested_device” checkbox to be submitted as the value of “reservation_requested_device”.</p>

<h2 id="applicable-attributes">Applicable Attributes</h2>

<p>Checkbox controls only use a subset of the typical <code>input</code> attributes. In particular, you’ll need to include</p>

<ul>
  <li><code>name</code> - This is the variable name you want to hold the user’s response. As mentioned in <a href="#multiple-choice-checkboxes">the previous section</a>, appending “[]” to the variable name will allow the variable to hold all of the user’s choices as opposed to only the final one.</li>
  <li><code>value</code> - This is the value that should be captured if the user ticks the checkbox.</li>
  <li><code>id</code> - The unique identifier you’re using for the control in order to explicitly associate it with a <code>label</code>.</li>
</ul>

<p>There are a few optional attributes you might consider including as well.</p>

<ul>
  <li><code>checked</code> - Use this null attribute if you want the default state of the checkbox to be ticked. This attribute should be used with caution. <strong>Don’t</strong> use this attribute to automatically check confirmation boxes for things like mailing list opt-ins. <strong>Do</strong> use this attribute when you are displaying sensible default settings or displaying confirmations the user has already made (e.g. in the user’s profile or when re-displaying the form when it has a submission error).</li>
  <li><code>required</code> - Use this to indicate the checkbox must be ticked for the form to be valid. It’s important to note that this attribute is typically only useful in confirmation checkbox scenarios. If you need a user to choose at least one from a multiple choice checkbox collection, it’s useless unless you need them to pick a specific one. To require one (or more) of a multiple choice checkbox group, you currently need to use JavaScript, like <a href="https://github.com/easy-designs/easy-checkbox-required.js">the one the Chattanooga Open Device Lab uses</a>.</li>
</ul>

<h2 id="checkbox-vs-other-controls">Checkbox vs. Other Controls</h2>

<p>Checkboxes excel at allowing users to indicate preference from a pre-defined set of options. But there are other form control types that allow for similar control over user responses. That can make it difficult to decide which element to use.</p>

<h3 id="dropdown-list-select">Dropdown List (<code>select</code>)</h3>

<p>The <code>select</code> element is another tried and true option for allowing users to indicate preference. A simple two-choice <code>select</code> could achieve the same goal as a confirmation checkbox, but it’s a little clunkier. In terms of user interface, <code>select</code> elements require more clicks of your users. They also obscure the complete list of choices from view because only one options is displayed at a time. Their appearance makes them more compact, but can make it difficult to get a complete picture of what choices are available when you can’t see them all.</p>

<p>You can enable multiple choice in a  <code>select</code> element by adding the <code>multiple</code> attribute to it, but depending on the number of <code>option</code> elements, it could also be a little unwieldy. Depending on the size of the <code>select</code> and the number of options, you could also create an inner scroll that could be awkward on certain touch-based devices.</p>

<p>The <code>select</code> element has its place, but should be used sparingly. I’ll go in-depth with <code>select</code> elements in a future post.</p>

<h3 id="choose-one-inputtyperadio">Choose One (<code>input[type=radio]</code>)</h3>

<p>For simple confirmation questions, it’s completely valid to use a radio form control in lieu of a single checkbox. In fact, in some cases, it may offer a more explicit choice for your users. Consider <a href="#fig-2016-01-06-03">this example</a> from <a href="https://order.subway.com">Subway’s online ordering tool</a>.</p>

<figure id="fig-2016-01-06-03" class="media-container">{% adaptive_image /i/posts/2016-01-06/03.png %}<figcaption>In this excerpt from Subway’s online ordering tool, they use a checkbox to confirm the user wants their sandwich toasted.</figcaption></figure>

<p>A checkbox labelled “Fresh Toasted”, isn’t terribly clear. A better approach would be to ask something like “Would you like your sandwich toasted?” with radio controls for “yes” and “no”. Alternately, if they absolutely wanted to keep it as a checkbox, they could use a better label: “Please toast my sandwich”.</p>

<figure id="fig-2016-01-06-04" class="media-container">{% adaptive_image /i/posts/2016-01-06/04.png %}<figcaption>An alternate approach to the Subway interface, using radio controls.</figcaption></figure>

<p>Radio controls have their place, but are not often a one-to-one replacement for checkboxes. I will discuss radio controls in greater depth in another post.</p>

<h2 id="check-em-out">Check ’Em Out</h2>

<p>Checkboxes are an invaluable tool in the form building tool chest. Understanding their purpose and capabilities is key to using them properly and ensuring your forms are usable to the broadest number of users.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Labeled With Love]]></title>
    <link href="https://www.aaron-gustafson.com/notebook/labeled-with-love/"/>
    <updated>2015-11-12T02:05:33+00:00</updated>
    <id>https://www.aaron-gustafson.com/notebook/labeled-with-love</id>
    <content type="html"><![CDATA[<p>Forms exist on pretty much every site on the web in one form or another. They are the primary mechanism by which we gather information from our users.<sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup> Of course, before anyone can fill out a form, they need to know what it’s asking for. Labeling is key.</p>

<!-- more -->

<p>A few months back, I relayed <a href="https://www.aaron-gustafson.com/notebook/consider-how-your-forms-read/">a story from Facebook</a> about how important the wording of their questions was in getting accurate responses from their users. The words we choose are incredibly important—your interface is a conversation with your users. I highly recommend reading up on that (and <a href="http://www.radiolab.org/story/trust-engineers/">listening to the Radiolab episode</a> that spurred me to write it), but I’m going to spend the remainder of this post talking about the utilitarian aspects of labels and how to use them properly in your forms.</p>

<h2 id="connecting-the-dots">Connecting the Dots</h2>

<p>When you look at a basic form field, you have two bits of information: the field and the label.</p>

<figure id="fig-2015-11-11-01" class="media-container">{% adaptive_image /i/posts/2015-11-11/01.png %}<figcaption>A typical form control: a label and a field.</figcaption></figure>

<p>You could achieve this with a minimum of markup:</p>

<p>{% gist 3585c019108025b2f568 unlabeled-field.html embed %}</p>

<p>The thing is, the text “Your Name” is not associated in any way with the <code>input</code>. Sure, a sighted person would likely be able to tell that that text is associated with the field, but no computer can tell that. And if a computer can’t tell the text and <code>input</code> are associated, your form control is inaccessible to anyone who uses assistive technology like a screen reader. It’s also going to pose a problem in the near-future of “headless UIs” like those hinted at by Cortana, Siri, and the Echo.</p>

<p>Thankfully, establishing a relationship between the two is quite easy using the <code>label</code> element. The most common (and preferable) way to do this is to wrap the labeling text in a <code>label</code> element. Then you create an explicit association with the field using the <code>for</code> attribute, which is an <code>id</code> reference. In other words, the value of the <code>for</code> attribute needs to match the value of the <code>id</code> attribute on the field you want to associate with that <code>label</code>.</p>

<p>{% gist 3585c019108025b2f568 labeled-field.html embed %}</p>

<p>With that markup in place, the programmatic connection between the elements is made and the results speak for themselves: When you focus the field, the contents of the <code>label</code> are read out.</p>

<p>{% youtube WR4_MAjalsU %}</p>

<h2 id="an-alternate-approach">An Alternate Approach</h2>

<p>Since I specifically referred to this approach as <em>explicit</em> association, you probably assumed that there’s another kind of association. And you were right: <em>implicit</em> association. Implicit association is created by wrapping a form control and its associated label text in a <code>label</code> element. I like to use this approach with radio and checkbox controls:</p>

<p>{% gist 3585c019108025b2f568 implicitly-labeled-checkbox.html embed %}</p>

<p>It’s worth noting that there’s nothing wrong with explicit association in this context either.</p>

<p>{% gist 3585c019108025b2f568 explicitly-labeled-checkbox.html embed %}</p>

<p>You can even combine the two approaches.</p>

<p>{% gist 3585c019108025b2f568 combo-labeled-checkbox.html embed %}</p>

<p>The reason I like to use implicit association with checkbox and radio controls has to do with ensuring the greatest breadth of support when it comes to styling inputs. For instance, if I set <code>width: 80%</code> on all <code>input</code> elements using a simple <a href="https://developer.mozilla.org/docs/Web/CSS/Type_selectors">type selector</a>, that width would be applied to <em>all</em> <code>input</code> elements, including radio and checkbox controls. In order to prevent radio and checkbox controls from getting rendered at that width, I would need to assign an override value of <code>width: auto</code> to them them specifically. I can do that using <a href="https://developer.mozilla.org/docs/Web/CSS/Attribute_selectors">attribute selectors</a>:</p>

<p>{% gist 3585c019108025b2f568 modern-only.css embed %}</p>

<p>While completely valid, that approach leaves out any browsers that don’t support attribute selection (e.g. IE 6). That may not seem like a deal-breaker in your book, but on the off chance some poor soul happens to be stuck using an out-of-date browser (as many are on mobile), I like to show them a little love. And, thankfully, using the implicit markup pattern for checkboxes and radio controls allows for this quite easily: I just use a <a href="https://developer.mozilla.org/docs/Web/CSS/Descendant_selectors">descendent selector</a> instead.</p>

<p>{% gist 3585c019108025b2f568 universal.css embed %}</p>

<p>This approach results in a greater amount of support and, incidentally, less CSS.</p>

<h2 id="added-benefit-interactivity">Added Benefit: Interactivity</h2>

<p>Obviously, associated labels are great for folks who use screen readers, but they have another benefit: tapping on a <code>label</code> will focus or activate the associated form control.</p>

<figure id="fig-2015-11-11-02" class="media-container"><img src="https://www.aaron-gustafson.com/i/posts/2015-11-11/02.gif" alt="" /><figcaption>Animation showing how clicking a <code>label</code> will focus the associated form control.</figcaption></figure>

<p>This isn’t a game-changer when it comes to standard text fields, but it’s an exceptional affordance when it comes to radio and checkbox controls, especially on mobile, as it vastly increases the tappable region used to activate the control.</p>

<figure id="fig-2015-11-11-03" class="media-container">{% adaptive_image /i/posts/2015-11-11/03.png %}<figcaption>A screenshot of a group of checkbox controls with their labels outlined.</figcaption></figure>

<p>To create incredibly generous tap targets on mobile devices, we can take things a little further. Add padding to the top and bottom of the <code>label</code> to make it bigger and then use negative margins to counter that enlargement and keep the layout as it was before the padding was applied.</p>

<p>{% gist 3585c019108025b2f568 larger-labels.css embed %}</p>

<figure id="fig-2015-11-11-04" class="media-container"><img src="https://www.aaron-gustafson.com/i/posts/2015-11-11/04.gif" alt="" /><figcaption>An animation showing very generous tap targets on a narrow screen.</figcaption></figure>

<p>It’s worth noting that older versions of Internet Explorer only provide the focus/interaction benefit when you use explicit label association. That’s why I like the combo approach of implicit <em>and</em> explicit association for checkbox and radio controls.</p>

<p><ins datetime="2015-12-09" cite="#comment-2374683375">As Dennis Lembrée mentions in the comments below, Dragon’s Naturally Speaking also doesn’t recognize implicit association, which is why it’s incredibly important to use explicit association even if it seems implicit association should suffice.</ins></p>

<h2 id="placeholders-arent-labels">Placeholders Aren’t Labels</h2>

<p>HTML5 ushered in a new option for working with <code>input</code> elements: the <code>placeholder</code> attribute. This declarative attribute makes it possible to offer hint as to the sort of content you were looking for in a field. In <a href="http://caniuse.com/#feat=input-placeholder">supporting browsers</a>, it appears in the field, ghosted back a bit, and disappears when you start typing a response.<sup id="fnref:2"><a href="#fn:2" class="footnote">2</a></sup></p>

<figure id="fig-2015-11-11-05" class="media-container"><img src="https://www.aaron-gustafson.com/i/posts/2015-11-11/05.gif" alt="" /><figcaption>An animation showing a placeholder in action on <a href="http://webstandardssherpa.com"><cite>Web Standards Sherpa</cite></a>.</figcaption></figure>

<p>Having this natively supported in the browser was a huge boon. For years we’d been using JavaScript to achieve this very effect—albeit typically for label text—in an effort to create more compact forms. Now we get the effect without having to include any additional files or libraries.</p>

<p>Of course, since <code>placeholder</code> implements an existing pattern, it came with baggage. People commonly achieved this effect by (ab)using the <code>value</code> attribute as a fake label. As such, its introduction didn’t do much to increase the accessibility of forms. <em>Form controls need a label</em>. If you want to make your form more compact, you can do that using proper markup and a little clever CSS.</p>

<p>{% gist 3585c019108025b2f568 fancy-example.html embed %}</p>

<p>{% gist 3585c019108025b2f568 fancy-example.css embed %}</p>

<figure id="fig-2015-11-11-06" class="media-container">{% codepen BoGgYM aarongustafson result 112 preview %}</figure>

<p>Mary Lou assembled some beautiful examples of this approach in her <a href="http://tympanus.net/codrops/2015/01/08/inspiration-text-input-effects/">Inspiration for Text Input Effects</a>. I highly recommend you check those out, but here’s a teaser to whet your whistle:</p>

<figure id="fig-2015-11-11-07" class="media-container"><img src="https://www.aaron-gustafson.com/i/posts/2015-11-11/07.gif" alt="" /><figcaption>A fancy, accessible form field and label from Mary Lou’s collection.</figcaption></figure>

<hr />

<p>We don’t have a ton of elements in HTML, which is why it’s important that we properly use the ones we do have. Hopefully this has provided a helpful overview of how to properly label form controls using HTML.</p>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>When we’re not, you know, tracking them with a <a href="http://arstechnica.com/security/2015/10/verizons-zombie-cookie-gets-new-life/">super cookie</a> or something. <a href="#fnref:1" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>Browsers typically exhibit two different behaviors here. Some hide the placeholder text as soon as you focus the field, others hide it only when you start typing. Either one works although, admittedly, I favor the text disappearing when you type rather than when the field receives focus. I can see how that approach might confuse some users, I just prefer it because it ensures you see the placeholder. <a href="#fnref:2" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Harvard, MIT, and Captioning]]></title>
    <link href="https://www.aaron-gustafson.com/notebook/harvard-and-mit-and-captioning/"/>
    <updated>2015-06-29T19:23:28+01:00</updated>
    <id>https://www.aaron-gustafson.com/notebook/harvard-and-mit-and-captioning</id>
    <content type="html"><![CDATA[<p>The U.S. Department of Justice (DOJ) has published Statements of Interest in two cases brought by the National Association of the Deaf (NAD) against <a href="http://www.ada.gov/briefs/harvard_soi.pdf">Harvard (PDF)</a> and <a href="http://www.ada.gov/briefs/mit_soi.pdf">MIT (PDF)</a>, respectively. The NAD is suing the two universities for violations of Title III of the Americans with Disabilities Act (ADA) and Section 504 of the Rehabilitation Act because the video and audio materials they are making available as part of their online learning offerings are not captioned.</p>

<!-- more -->

<p>The DOJ Statements make it quite clear that</p>

<blockquote>
  <p>Both the ADA and Section 504 currently obligate Harvard to provide effective communication to ensure equal access to its online programming services, and resolution of Plaintiffs’ claim involves a straightforward application of longstanding statutory and regulatory requirements. For more than two decades, federal courts have resolved effective communication claims brought under the ADA and Section 504 in a wide range of contexts, including claims alleging unequal access to goods, benefits and services provided through websites or other electronic media. And the Departments of Justice and Education have routinely required covered entities to ensure equal access to goods, benefits and services, electronic or otherwise, through the provision of captioning or other auxiliary aids or services.</p>
</blockquote>

<p>Also…</p>

<blockquote>
  <p>[T]he Department issued an Advanced Notice of Proposed Rulemaking (“ANPRM”) on Accessibility of Web Information and Services of State and Local Government Entities and Public Accommodations, announcing the Department’s interest in developing more specific requirements or technical standards for  website accessibility. … In the ANPRM, the Department reaffirmed its longstanding position that the ADA applies to websites of public accommodations, and reiterated, consistent with the preamble to the 1991 regulations, that the ADA regulations should be interpreted to keep pace with developing technologies.</p>
</blockquote>

<p>Neither case has been settled yet, but the fact that the DOJ is siding with the NAD will lend more credence to their complaints and will likely result in one or both institutions settling out of court and, eventually, captioning their videos.</p>

<p>Captions are critical for the deaf and hard of hearing as they let them know what’s being said, who is saying it, how it’s being said, and inform them of any other sounds that are germane to the content. Reading captions is the equivalent of hearing with your eyes.</p>

<p>Are your videos captioned? If your content is aimed at a broad audience, it might be worth your time (and even your money) to get them captioned.</p>

<p>No doubt, accurate captioning is time-consuming. There are some ways of automating it (<a href="https://support.google.com/youtube/answer/3038280">YouTube does this</a>, for instance), but there are also services like <a href="https://castingwords.com/">Casting Words</a>, <a href="https://www.rev.com/caption">Rev</a>, and <a href="http://www.3playmedia.com/">3Play Media</a> that will do it for a nominal fee.</p>

<p>It’s worth noting that the HTML <code>video</code> element supports subtitles and captions via <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/track">the <code>track</code> element</a> (<a href="http://www.iandevlin.com/blog/2015/04/html5/html5-video-captions-current-browser-status">browser support</a>, <a href="http://caniuse.com/#search=track">Can I Use data</a>). YouTube also allows you to <a href="https://support.google.com/youtube/answer/2734796">add captions and subtitles to your videos manually</a>. But keep in mind that <a href="http://screenfont.ca/learn/">captions and subtitles are not the same thing</a>; captions need to capture more than just the words that are said.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Where Do We Go From Here?]]></title>
    <link href="https://www.aaron-gustafson.com/notebook/where-do-we-go-from-here/"/>
    <updated>2015-06-22T16:49:56+01:00</updated>
    <id>https://www.aaron-gustafson.com/notebook/where-do-we-go-from-here</id>
    <content type="html"><![CDATA[<p><em>I had the great pleasure of delivering the closing keynote for the final Responsive Day Out. Here’s what I had to say.</em></p>

<!-- more -->

<hr />

<p>Today has provided an amazing tour of the world of responsive design. We’ve seen how to level-up our workflows and processes. We’ve learned new ways to improve the accessibility of our products. We’ve grappled with modern CSS and HTML capabilities that help us embrace the hugely variable display sizes that swirl and whirl around us.</p>

<p>We’ve explored the future of modular code and browsers’ capacity for working without network connectivity. And we’ve even taken a trip into the possible future of where the web might go.</p>

<figure id="figure-2015-06-22-01">{% adaptive_image /i/posts/2015-06-22/01.jpg %}</figure>

<p>We’ve come a long way since <a href="https://huffduffer.com/adactio/243780">Ethan’s article</a>, fluid grids, flexible media, and media queries. Those three tenets sowed a seed that has grown and flourished as we have come to better understand the implications of device proliferation. We’ve seen that the web is capable of going anywhere and doing pretty much anything.</p>

<p>I would argue that <a href="https://huffduffer.com/adactio/243780">“Responsive Web Design”</a> was the first article that really managed to capture the concepts that John Allsopp had discussed so many years before in <a href="http://www.alistapart.com/articles/dao/">“A Dao of Web Design”</a> and distilled them into something the design and development community could really sink their teeth into. It provided a concrete example of the web’s ability to flex and mold itself into whatever shape it needed to take on.</p>

<p>It was the first time many designers had come to terms with the idea that “experience” was not some monolithic thing.</p>

<p>Sure, many of us in the web standards community had been talking the talk and walking the walk with regard to <a href="http://alistapart.com/article/understandingprogressiveenhancement">progressive enhancement</a>. And we were gaining converts, but progress was slow. Ethan demonstrated—directly and succinctly—what the progressive enhancement of visual design could look like.</p>

<p>Providing an identical experience for each and every human being that tries to access our sites would be impossible. There are simply far too many factors to consider. We’ve got screen size, display density, CPU speed, amount of RAM, sensor availability, feature availability, interface methods … <em>breathe</em> … operating system type, operating system version, browser type, browser version, plug-ins installed, network speed, network latency, network congestion, firewalls, proxies, routers, and probably a dozen other factors my mind is incapable of plucking amid the whirlwind of technical considerations.</p>

<p><strong>And that doesn’t even consider our users.</strong></p>

<p>When it comes to the people we need to reach for our work to actually matter, we have to consider literacy level, reading acumen, level of domain knowledge, cognitive impairments like learning disabilities and dyslexia, attention deficit issues, environmental distractions, vision impairment, hearing impairment, motor impairment, how much they understand how to use their device, how much they understand how to use their browser, how well-versed in common web conventions they are, and a ton of other “human factors”.</p>

<p>Every person is different and everyone comes to the web with their own set of special needs. Some are always with them, blindness for example. Others are transient, like breaking your mousing arm. Still others are purely situational and dependent on the device you are using at the time and its technical capabilities or constraints.</p>

<p>Trying to devise one monolithic experience for each and every person to have in every context that considers every factor would be impossible. And yet, Sir Tim Berners Lee had a vision for a web that was capable of going anywhere. Was he insane?</p>

<p><a href="http://www.w3.org/History/1989/proposal.html">Sir Tim’s vision for the web</a> was that content could be created once and accessed from anywhere. Disparate but related pieces of “hypermedia” scattered across the globe could be connected to one another via links. Moreover, they would be retrievable by anyone on any device capable of reading HTML. For free.</p>

<p><strong>Ultimately, Sir Tim envisioned universal accessibility.</strong></p>

<p>For a great many of us, ensuring our websites are accessible is an afterthought. We talk a good game when it comes to “user centered” this or that, but often treat the word “accessibility” as a synonym for “screen reader”. It’s so much more than that. “Accessibility” is about people. People consume content and use interfaces in many different ways, some similar and some quite dissimilar to how we do it.</p>

<p>Sure, people with visual impairments often use a screen reader to consume content. But they might also use a braille touch feedback device or a braille printer. They probably also use a keyboard. Or they may use a touchscreen in concert with audio cues. Or they may even use a camera to allow them to “read” content via OCR and text-to-speech. And yes, visual impairment affects a decent percentage of the populace (especially as we age), but it is only part of the “accessibility” puzzle.</p>

<p>The contrast between text and the background is an important factor in ensuring content remains readable in different lighting situations. Color choice is an accessibility concern.</p>

<p>The language we use on our sites and in our interfaces directly affects how easy it is for our users to understand what we do, the products we are offering, and why it matters. It also affects how we make our users feel about themselves, their experience, and our companies. Language is an accessibility concern.</p>

<p>The size of our web pages has a direct effect on how long our pages take to download, how much it costs our customers to access them, and (sometimes) even whether or not the content can be reached. Performance is an accessibility concern.</p>

<p>I could keep going, but I’m sure you get the point.</p>

<p>Accessibility is about providing good experiences for everyone, regardless of physical or mental abilities, gender, race, or language. It recognizes that we all have special needs—physical limitations, bandwidth limitations, device limitations—that may require us to  experience the same interface in different ways.</p>

<p>When I visit a website on my phone, for example, I am visually limited by my screen resolution (especially if I am using a browser that encourages zooming) and I am limited in my ability to interact with buttons and links because I am browsing with my fingertips, which are larger and far less accurate than a mouse cursor.</p>

<p>On a touchscreen, I may need the experience to be slightly different, but I still need to be able to do whatever it is I came to the site to do. I need <em>an</em> experience, but moreover I need the <em>appropriate</em> experience.</p>

<p>Embracing the reality that experience does’t need to be just one thing will help us reach more people with fewer headaches. Experience can—and should—be crafted as a continuum. This is progressive enhancement: We start with a baseline experience that works for everyone—content, real links, first generation form controls, and forms that actually submit to the server. Then we build up the experience from there.</p>

<figure id="figure-2015-06-22-02"><img src="https://www.aaron-gustafson.com/i/posts/2015-06-22/02.gif" alt="" /></figure>

<p>Your browser supports HTML5 form controls? Great! You’ll get a better virtual keyboard when you go to type your email address. You can use CSS? Awesome, let me make that reading experience better for you. Oh, you can handle media queries! Let me adjust the layout so those line lengths are a little more comfortable. Wow, your browser supports Ajax?! Here let me load in some teasers for related content you might find interesting.</p>

<p>Imagine sitting down in a restaurant only to have the waiter immediately bring you a steak. But you’re a vegetarian. You ask if they offer something you can eat and they politely reply <em>Oh I’m sorry, meat is a requirement. Why don’t you just eat meat? It’s easy! You’re really missing out on some tasty food.</em> No waiter who actually cares about your experience would do that.</p>

<p>And yet we—as an industry—don’t seem to have any problem telling someone they need to change their browser to accommodate us. That’s just wrong. Our work is meaningless without users. We should be bending over backwards to attract and retain them. This is customer service 101.</p>

<p>This comes back to Postel’s law, which Jeremy often recounts:</p>

<blockquote>
  <p>Be conservative in what you do, be liberal in what you accept from others.</p>
</blockquote>

<p>We need to be lax when it comes to browser support and not make to many (or better yet any) assumptions about what we can send.</p>

<p>Of course this is not an approach everyone in our industry is ready to embrace, so I’ll offer another quote I come back to time and time again…</p>

<blockquote>
  <p>When something happens, the only thing in your power is your attitude toward it; you can either accept it or resent it.</p>
</blockquote>

<p>We can’t control the world, we can only control our reaction to it.</p>

<p>Now those of you who’ve gathered for this final Responsive Day Out (or who are following along at home) probably understand this more than most. We feel the constant bombardment of new devices, screen sizes, and capabilities. The only way I’ve found to deal with all of this is to accept it, embrace the diversity, and view device and browser proliferation as a feature, not a bug.</p>

<p>It’s up to us to educate those around us who have—either by accident or intent—not accepted that diversity is the reality we live in and things are only going to get crazier. Burying our heads in the sand is not an option.</p>

<p>When I am trying to help folks understand and embrace diversity, I often reach for one of my favorite thought exercises from <a href="https://en.wikipedia.org/wiki/John_Rawls">John Rawls</a>.</p>

<figure id="figure-2015-06-22-03">{% adaptive_image /i/posts/2015-06-22/03.jpg %}</figure>

<p>Rawls was a philosopher who used to run a social experiment with students, church groups, and the like.</p>

<p>In the experiment, participants were allowed to create their ideal society. It could follow any philosophy: It could be a monarchy or democracy or anarchy. It could be capitalist or socialist. The people in this experiment had free rein to control absolutely every facet of the society… but then he’d add the twist: They could not control what position they occupied in that society.</p>

<p>This twist is what <a href="https://en.wikipedia.org/wiki/John_Harsanyi">John Harsanyi</a>—an early game theorist—refers to as the <a href="https://en.wikipedia.org/wiki/Veil_of_ignorance">“Veil of Ignorance”</a> and what Rawls found, time and time again, was that individuals participating in the experiment would gravitate toward creating the most egalitarian societies.</p>

<p>It makes sense: what rational, self-interested human being would treat the elderly, the sick, people of a particular gender, race, creed, or color poorly if they could find themselves in that very same position when the veil is pulled away?</p>

<p>The things we do to accommodate special needs now pay dividends in the future. Look at ramps.</p>

<figure id="figure-2015-06-22-04">{% adaptive_image /i/posts/2015-06-22/04.jpg %}</figure>

<p>They’re a classic example of an accessibility feature for people in wheelchairs that also benefit people who aren’t in them: People toting luggage, delivery services hauling heavy things on dollies, parents pushing children (or their dressed up dogs) in strollers, a commuter walking her bike, and that guy who just prefers walking up a gentle incline to expending the effort required to mount a step.</p>

<p>When we create alternative paths to get from Point A to Point B, people can take the one most appropriate for them, whether by choice or necessity. And everyone can accomplish their goals.</p>

<p>We all have special needs. Some we’re born with. Some we develop. Some are temporary. Some have nothing to do with us personally, but are situational or purely dependent on the hardware we are using, the interaction methods we have available to us, or even the speed at which we can access the Internet or process data.</p>

<p>What is responsive web design about if not accessibility? Yes, its fundamental tenets are concerned with visual design, but in terms of the big picture, they’re all about providing the best possible reading experience.</p>

<p>As practitioners of responsive design, we understand the benefits of adapting our interfaces. We understand fallbacks. We understand how to design robust experiences that work under a wide variety of conditions. Every day we broaden the accessibility of our products.</p>

<p>These skills will make us invaluable as technology continues to offer novel ways of consuming and interacting with our websites.</p>

<p>We’re just starting to dip or toes—er, hands—into the world of motion-based gestural controls. Sure, we’ve had them in two dimensions on touch screens for a while now but three dimensional motion-based controls are only beginning to appear.</p>

<p>{% youtube VXhhE-l96qQ 41s/78s %}</p>

<p>The first big leap in this direction was <a href="https://en.wikipedia.org/wiki/Kinect">Kinect</a> on the Xbox 360 (and later, Windows). With Kinect, we interact with the computer using body movements like raising a hand (which gets Kinect to pay attention), pushing our hand forward to click/tap, and grasping to drag the canvas in a particular direction.</p>

<p>The Kinect ushered in a major revolution in terms of interfacing with computers, but from an interaction perspective, it presents similar challenges to those of the <a href="https://en.wikipedia.org/wiki/Wii#Wii_Remote">Wii controller</a> and Sony’s <a href="https://en.wikipedia.org/wiki/PlayStation_Move">PlayStation Move</a>. Large body gestures like raising your hand (or a wand controller) can be tiring.</p>

<p>{% youtube 21LtA5-wiwU 7s/19s %}</p>

<p>They’re also not terribly accurate. If you thought that touchscreen accuracy was an issue, hand gestures like those for the Kinect or <a href="https://en.wikipedia.org/wiki/Leap_Motion">LEAP Motion</a> pose even more of a challenge.</p>

<p>To accommodate interactions like this (which we currently have no way of detecting) we need to be aware of how easy it is to click on interactive controls. We need to determine if our buttons and links are large enough and whether there is enough space between them to ensure our user’s intent is accurately conveyed to the browser. Two specs which can help address this are Media Queries Level 4 and Pointer Events.</p>

<p>In <a href="http://dev.w3.org/csswg/mediaqueries-4/">Media Queries Level 4</a>, we became able to apply style rules to particular interaction contexts. For instance, when we have very accurate control over our cursor (as in the case of a stylus or mouse) or less accurate control (as in the case of a touch screen or physical gesture):</p>

<p>{% gist 372271534c78cf11d4a6 mq4-pointer.css embed %}</p>

<p>Of course, we want to offer a sensible default in terms of size and spacing as a fallback for older browsers and devices.</p>

<p>We also have the ability to determine whether the device is capable of hovering over an element and can adjust the interface accordingly.</p>

<p>{% gist 372271534c78cf11d4a6 mq4-hover.css embed %}</p>

<p>We still need to figure out how well all of this ends up working on multimodal devices like the Surface tablet, however. Will the design change as the user switches between input modes? Should it? To that end, the spec also provides <code>any-pointer</code> and <code>any-hover</code> to allow you to query for whether <em>any</em> supported interaction method meets your requirements, but here’s a word of warning from the spec:</p>

<blockquote>
  <p>Designing a page that relies on hovering or accurate pointing only because <code>any-hover</code> or <code>any-pointer</code> indicate that an input mechanism with these capabilities is available, is likely to result in a poor experience.</p>
</blockquote>

<p>These media query options are starting to roll out in Chrome, Mobile Safari, and Microsoft Edge, so it’s worth taking a look at them.</p>

<p><a href="http://www.w3.org/TR/pointerevents/">Pointer Events</a> is another spec that is beginning to gain some traction. It generalizes interaction to a single event rather than forcing us to silo experience into mouse-driven, touch-driven, pen-driven, (sigh) force-driven, and so on.</p>

<p>We can unobtrusively detect support for Pointer Events…</p>

<p>{% gist 372271534c78cf11d4a6 pointer-test.js embed %}</p>

<p>…and then handle them all in the same way or create branches based on the <code>pointerType</code>:</p>

<p>{% gist 372271534c78cf11d4a6 pointer-event.js embed %}</p>

<p>Of course, in addition to considering the level of accuracy our users have while interacting with our screens, we also need to consider the potentially increased distance at which our users are reading our content.</p>

<p>To that end, I’ve been experimenting with the viewport width (<code>vw</code>) unit.</p>

<p>For a long time, I’ve used ems for the layout’s <code>max-width</code> (so the line length is proportional to the font size). I also use relative font sizes. With that as the foundation, I can use a media query that matches the maximum width and set the base font size at the vw equivalent at the max width.</p>

<p>{% gist 372271534c78cf11d4a6 vw-scaling.css embed %}</p>

<p>Then the whole design will simply zoom the layout when viewed beyond that size.</p>

<p>{% youtube 6XoN9mMgI38 %}</p>

<p>If you don’t want to turn something like that on automatically, you can enable it to be toggled on and off with JavaScript.</p>

<p>{% youtube 96l_W7ca6SM %}</p>

<p>Things get even crazier when you start to factor in devices like the HoloLens. And no, I have not gotten to play with one yet.</p>

<p>{% youtube 3AADEqLIALk 87s/117s %}</p>

<p>But the idea of being able to drop a resizable virtual screen on any surface presents some interesting possibilities as a user and some unique challenges as a designer. HoloLens, of course, brings with it gesture controls as well, so accounting for a variety of input types should get us pretty far.</p>

<p>In a similar vein, we should begin to think about what experiences can and should look like when we are browsing solely with our gaze. Gaze tracking has its origins in the accessibility space as a means of providing interface control to folks with limited or no use of their hands. Traditionally, gaze tracking hardware has been several thousand dollars, putting it out of the reach of many people, but that is starting to change.</p>

<p>In the last few years, the computational power of our devices has increased as the hardware costs associated with supporting gaze tracking have dropped dramatically. Looking around, you can see gaze tracking beginning to move into the public sphere: Many smartphones and smartwatches can recognize when you are looking at them (or at least they do sometimes). This is only a short step away from knowing where on the screen you are looking. And nearly every high-end smartphone is now equipped with a front-facing camera which makes them perfect candidates to provide this interaction method.</p>

<p>{% youtube DEk7PlJWQgI 18s/54s %}</p>

<p>The <a href="http://sesame-enable.com/phone/">Sesame Phone</a> was designed to allow people to use a smartphone without using their hands. It uses facial tracking to move a virtual cursor around the screen, allowing users to interact with the underlying operating system as well as individual applications. It supports tap, swipe, and other gestures (via a context menu) and is pretty impressive in my experience. Technology like this enables people suffering from MS, arthritis, Muscular Dystrophy, and more to use a smartphone and—more importantly to us—browse the web.</p>

<p><a href="https://theeyetribe.com/">The Eye Tribe</a> and <a href="http://www.fixational.com/">Fixational</a> are similarly working to bring eye tracking to smartphones and tablets. Eye tracking is similar to face tracking, but the cursor follows your focus. Micro gestures—blink, wink, etc.—allow you to interact with the device.</p>

<p>Even though most gaze tracking software mimics a mouse and has adjustable sensitivity, the accuracy of it as a pointer device is not fantastic. When I’ve used the Sesame Phone, for instance, I’ve have a hard time controlling the position of my head in order to hold the cursor still to hover and click a button. I’m sure this would improve with practice, but it’s safe to say that in a gaze interaction, larger, well spaced, and more easily targeted links and buttons would be a godsend.</p>

<p>So far, I’ve focused on interaction methods that facilitate navigation and consuming content. But what about filling out a form? I can tell you that typing an email letter-by-letter on a virtual keyboard using your face, sucks…</p>

<p>Thankfully, most of these gestural implementations are coupled with some form of voice recognition. The Kinect, for instance, will accept verbal commands to navigate and accomplish tasks like filling in forms. The Sesame Phone also supports voice commands for certain basic actions, dictating email, and the like.</p>

<p>Coupled with voice, the alternative interaction methods of Kinect and Sesame Phone work really well. But voice interaction can stand on its own too.</p>

<p>Most of us are familiar with <a href="https://en.wikipedia.org/wiki/Siri">Apple’s Siri</a>, <a href="https://en.wikipedia.org/wiki/Google_Now">Google Now</a>, and <a href="https://en.wikipedia.org/wiki/Microsoft_Cortana">Microsoft’s Cortana</a>. These digital assistants are great at retrieving information from select sources and doing other assistant-y things like calculating a tip and setting a reminder. As far as interacting with the web, however, they don’t… yet. We can engage with them, but they can‘t (necessarily) engage with a web page.</p>

<p>Exposing the information stored in our webpages via semantic HTML and structured syntaxes like <a href="http://microformats.org/">microformats</a>, <a href="https://en.wikipedia.org/wiki/Microdata_(HTML)">microdata</a>, and <a href="https://en.wikipedia.org/wiki/RDFa">RDFa</a> <em>should</em> eventually make our content available to these assistants, but we don’t really know. Their various makers haven’t really given us any clue as to how to do that and, as it stands right now, none of them can look up a web page and read it to you. For that you need to invoke a screen reader.</p>

<p>Each company offers a first-party screen reader. And all are capable of helping you interact with a page, including helping you fill in forms, without having to see the page. And yet, these technologies have not been coupled with their corresponding assistants. It probably won’t be long before we see that happen.</p>

<p>When we start to consider how our websites will be experienced in a voice context, the readability of our web pages becomes crucial. Clear well-written prose and a logical source order will be an absolute necessity. If our pages don’t make sense when read, what’s the point?</p>

<p>Content strategist Steph Hay views interface as an opportunity to have a conversation with our users. Soon it literally will be.</p>

<p>Interestingly, Microsoft has given us a peek at what it might be like to design custom voice commands for our websites beyond what the OS natively supports with Cortana. In other words, they let us teach their assistant.</p>

<p>In Windows 10, installable web apps can include a <a href="https://msdn.microsoft.com/en-us/library/windows/apps/dn722331.aspx">Voice Command Definition (VCD) file</a> in the <code>head</code> of the page to enable custom commands:</p>

<p>{% gist 372271534c78cf11d4a6 vcd.html embed %}</p>

<p>The referenced VCD file is simply an XML file defining the keyword for the web app and commands that can be issued.</p>

<p>Using very basic syntax, The VCD identifies optional pieces of a given phrase and variables Cortana should extract:</p>

<p>{% gist 372271534c78cf11d4a6 vcd.xml embed %}</p>

<p>This particular app passes the captured information over to JavaScript for processing. That’s right, <a href="https://msdn.microsoft.com/en-us/library/dn722330.aspx#handle_activation_and_execute_voice_commands">Cortana has a JavaScript API too</a>. Pretty neat.</p>

<p>But traditional computers and smart mobile devices aren’t the only place we’re starting to see voice based experiences. We also have disembodied voices like <a href="https://en.wikipedia.org/wiki/Amazon_Echo">Amazon’s Echo</a> and <a href="http://www.theubi.com/">the Ubi</a> which are completely headless.</p>

<figure id="figure-2015-06-22-11">{% adaptive_image /i/posts/2015-06-22/05.jpg %}</figure>

<p>Right now, they both seem squarely focused on helping your house become “smarter”—streaming music, adjusting the thermostat, etc.—but it isn’t hard to imagine these devices becoming coupled with the ability to browse and interact with the web.</p>

<p>In the near future, voice-based interactions with the web will be entirely possible. They will likely suck a bit at first, but they’ll get better.</p>

<p>I’m going to make a somewhat bold prediction: while touch has been revolutionary in many ways toward improving digital access, voice is going to be even more significant. Voice-based interfaces will create new opportunities for people to interact with and participate in the digital world.</p>

<p>Because we’ve been thinking about how the experiences we create are consumable across a variety of devices, we’ve got the jump on other folks working on the web when it comes to voice. We see experience as a continuum, starting with text.</p>

<p>As voice technology matures, we will be the ones people look to as the experts. We will empower the next generation of websites and applications to become voice-enabled and in so doing, we will improve the lives of billions. Because “accessibility” is not about disabilities, it’s about access and <strong>it’s about people</strong>.</p>

<p>Sure, we’ll make it easier to look up movie times and purchase tickets to see the latest <cite>Transformers</cite> debacle, but we will also empower the nearly 900 million people globally—over 60% of whom are women—that are illiterate. And that’s a population that has been largely ignored on our dominantly textual web.</p>

<p>We will create new opportunities for the poor and disadvantaged to participate in a world that has excluded them. You may not be aware, but 80% of Fortune 500 companies—think Target, Walmart—only accept job applications online or via computers. We will enable people who have limited computer skills or who struggle with reading to apply for jobs with these companies.</p>

<p>We can help bridge the digital divide and the literacy gap. We can create opportunities for people to better their lives and the lives of their families. We have the power to create more equity in this world than most of us have ever dreamed.</p>

<p>This is an incredibly exciting time, not just for the responsive design community, not just the web, but for the world! The future is coming and I can’t wait to see how awesome you make it!</p>

<figure id="figure-2015-06-22-12">{% adaptive_image /i/posts/2015-06-22/06.jpg %}</figure>

<hr />

<p><em>Responsive Day Out 3: The Final Breakpoint was held in Brighton, UK on 19 June 2015.</em></p>

<ul>
  <li><a href="https://huffduffer.com/adactio/243780">Listen to this presentation on Huffduffer</a>.</li>
  <li>Read <a href="https://decadecity.net/blog/2015/06/19/aaron-gustafson-where-do-we-go-here">Orde Saunders’ notes</a> from my talk.</li>
  <li>Read <a href="https://hiddedevries.nl/en/blog/2015-06-20-responsive-day-out-3-the-final-breakpoint/">Hidde de Vries’ recap of the day</a>.</li>
</ul>
]]></content>
  </entry>
  
</feed>
